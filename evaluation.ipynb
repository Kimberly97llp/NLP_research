{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.1.1+cu118)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\yezix\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.7.22)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install tokenizers\n",
    "!pip install huggingface_hub\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "\n",
    "    # List all directories in the results folder\n",
    "    all_checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "    # Extract the checkpoint numbers\n",
    "    checkpoint_numbers = [int(d.split(\"-\")[1]) for d in all_checkpoints if d.startswith(\"checkpoint-\")]\n",
    "    # Find the folder name of the latest checkpoint\n",
    "    latest_checkpoint_folder = f\"{checkpoint_dir}/checkpoint-{max(checkpoint_numbers)}\"\n",
    "    \n",
    "    return latest_checkpoint_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(checkpoint_dir):\n",
    "    checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(dataset_dir):\n",
    "    dataset_train_path = f\"{dataset_dir}/train.jsonl\"\n",
    "    #dataset_test_path = f\"{dataset_dir}/test.jsonl\"\n",
    "    dataset_test_path = f\"{dataset_dir}/test_small.jsonl\"\n",
    "    dataset_train = load_dataset('json', data_files=dataset_train_path)['train']\n",
    "    dataset_test = load_dataset('json', data_files=dataset_test_path)['train']\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Given the description after \"Description:\", complete the last sentence with a true statement about the contents\n",
    "of the specified box according to the description.\n",
    "Description: \n",
    "\"\"\"\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for entry in tqdm(dataset):\n",
    "        # tokenize problem\n",
    "        input_data = PROMPT + entry['sentence_masked'][:-15] + \":\"\n",
    "        input_encoding = tokenizer(input_data, truncation=True, padding='max_length', max_length=512, return_attention_mask = True, return_tensors = 'pt')\n",
    "        input_ids.append(input_encoding['input_ids'])\n",
    "        attention_masks.append(input_encoding['attention_mask'])\n",
    "         # tokenize answer\n",
    "        target = entry['masked_content'][13:]\n",
    "        target_encoding = tokenizer(target, truncation=True, padding='max_length', max_length=512, return_attention_mask = True, return_tensors = 'pt')\n",
    "        target_input_ids = target_encoding['input_ids']\n",
    "        target_input_ids[target_input_ids == tokenizer.pad_token_id] = -100\n",
    "        labels.append(target_input_ids)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    tokenized_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def make_dataloader(tokenized_dataset):\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five-Shot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_dir):\n",
    "    checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader):\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = 1e-4, eps = 1e-8)\n",
    "\n",
    "    for _ in tqdm(range(3)):\n",
    "\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "\n",
    "          input_ids = batch[0].to(device)\n",
    "          input_mask = batch[1].to(device)\n",
    "          labels = batch[2].to(device)\n",
    "          \n",
    "          model.zero_grad()\n",
    "          outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "          loss = outputs.loss\n",
    "          total_train_loss += loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "          optimizer.step()\n",
    "    \n",
    "    print(f\"Total train loss: {total_train_loss}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_all(model, tokenizer, dataloader):\n",
    "    model_predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # Generate sequences for the batch\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        sequence_ids = model.generate(input_ids, attention_mask=input_mask)\n",
    "        sequences = tokenizer.batch_decode(sequence_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Accumulate generated sequences\n",
    "        model_predictions.extend(sequences)\n",
    "\n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_to_data(model_preds, model_name, dataset):\n",
    "    dataset = dataset.add_column(f\"model_{model_name}_pred\", model_preds)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name=\"base\", zeroshot=True):\n",
    "\n",
    "    # prepare datasets\n",
    "    if (model_name == \"base\"):\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    else:\n",
    "        tokenizer = load_tokenizer(f\"./results/{model_name}/\")\n",
    "    dataset_train, dataset_test = load_datasets(\"./eval/current_use\")\n",
    "    tokenized_dataset_train = tokenize_dataset(dataset_train, tokenizer)\n",
    "    tokenized_dataset_test = tokenize_dataset(dataset_test, tokenizer)\n",
    "    train_loader = make_dataloader(tokenized_dataset_train)\n",
    "    test_loader = make_dataloader(tokenized_dataset_test)\n",
    "\n",
    "    # load and potentially train model\n",
    "    if (model_name == \"base\"):\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "    else:\n",
    "        model = load_model(f\"./results/{model_name}/\")\n",
    "    if not zeroshot:\n",
    "        train_model(model, train_loader)\n",
    "\n",
    "    # make predictions\n",
    "    model_preds = make_prediction_all(model, tokenizer, test_loader)\n",
    "    dataset_test = add_prediction_to_data(model_preds, model_name, dataset_test)\n",
    "    dataset_test.to_json(f\"./results/eval/preds_{model_name}_small.jsonl\", orient=\"records\")\n",
    "    #dataset_test.to_json(f\"./results/eval/preds_{model_name}.jsonl\", orient=\"records\")\n",
    "    \n",
    "    # garbage collection\n",
    "    del model, tokenizer, tokenized_dataset_train, tokenized_dataset_test, train_loader, test_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model_preds, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 5/5 [00:00<00:00, 833.23it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1904.30it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.86s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.83s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.54s/it]\n",
      "100%|██████████| 3/3 [00:17<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 2.1575117111206055.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 13/13 [01:12<00:00,  5.60s/it]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 58.82ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_math, dataset_test_math = evaluate_model(model_name=\"math\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_math_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 5/5 [00:00<00:00, 1667.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1169.47it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.72s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.60s/it]\n",
      "100%|██████████| 3/3 [00:18<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 1.92369544506073.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 13/13 [01:26<00:00,  6.68s/it]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 500.16ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_code, dataset_test_code = evaluate_model(model_name=\"code\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_code_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 5/5 [00:00<00:00, 833.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1155.94it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.49s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 2.442920446395874.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 13/13 [01:12<00:00,  5.60s/it]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 499.92ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_base, dataset_test_base = evaluate_model(model_name=\"base\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_base_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_all = Dataset.from_dict({\n",
    "    # problem and solution\n",
    "    'sentence_masked': dataset_test_base['sentence_masked'],\n",
    "    'masked_content': dataset_test_base['masked_content'],\n",
    "    # predictions\n",
    "    'model_base_pred': model_preds_base,\n",
    "    'model_math_pred': model_preds_math,\n",
    "    'model_code_pred': model_preds_code,\n",
    "    # metadata\n",
    "    'sample_id': dataset_test_base['sample_id'],\n",
    "    'numops': dataset_test_base['numops'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence_masked', 'masked_content', 'model_base_pred', 'model_math_pred', 'model_code_pred', 'sample_id', 'numops'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_masked': ['Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 5 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 6 contains <extra_id_0> .',\n",
       "  'Box 0 contains the cash and the ticket, Box 1 contains nothing, Box 2 contains the beer and the brain and the guitar, Box 3 contains the medicine and the newspaper, Box 4 contains the crown, Box 5 contains the ice and the painting, Box 6 contains the bomb and the boot and the plane. Put the meat into Box 5. Move the ticket from Box 0 to Box 1. Remove the newspaper from Box 3. Remove the guitar from Box 2. Remove the brain from Box 2. Remove the beer from Box 2. Move the ticket from Box 1 to Box 3. Move the medicine from Box 3 to Box 2. Move the cash from Box 0 to Box 3. Box 0 contains <extra_id_0> .',\n",
       "  'Box 0 contains the cash and the ticket, Box 1 contains nothing, Box 2 contains the beer and the brain and the guitar, Box 3 contains the medicine and the newspaper, Box 4 contains the crown, Box 5 contains the ice and the painting, Box 6 contains the bomb and the boot and the plane. Put the meat into Box 5. Move the ticket from Box 0 to Box 1. Remove the newspaper from Box 3. Remove the guitar from Box 2. Remove the brain from Box 2. Remove the beer from Box 2. Move the ticket from Box 1 to Box 3. Move the medicine from Box 3 to Box 2. Move the cash from Box 0 to Box 3. Box 1 contains <extra_id_0> .',\n",
       "  'Box 0 contains the cash and the ticket, Box 1 contains nothing, Box 2 contains the beer and the brain and the guitar, Box 3 contains the medicine and the newspaper, Box 4 contains the crown, Box 5 contains the ice and the painting, Box 6 contains the bomb and the boot and the plane. Put the meat into Box 5. Move the ticket from Box 0 to Box 1. Remove the newspaper from Box 3. Remove the guitar from Box 2. Remove the brain from Box 2. Remove the beer from Box 2. Move the ticket from Box 1 to Box 3. Move the medicine from Box 3 to Box 2. Move the cash from Box 0 to Box 3. Box 2 contains <extra_id_0> .'],\n",
       " 'masked_content': ['<extra_id_0> the dish',\n",
       "  '<extra_id_0> the cake and the television',\n",
       "  '<extra_id_0> nothing',\n",
       "  '<extra_id_0> nothing',\n",
       "  '<extra_id_0> the medicine'],\n",
       " 'model_base_pred': ['box 0 contains the boat and the car and the plane, box 1 contains the cake,',\n",
       "  '0 contains the boat and the shoe and the television, Box 0 contains the camera and',\n",
       "  'the beer and the brain and the guitar, Box 1 contains nothing, Box 2 contains the cash',\n",
       "  'Box 0 contains nothing, Box 1 contains nothing, Box 2 contains nothing, Box 3 contains',\n",
       "  'box 0 contains: the cash and the ticket, Box 0 contains: the cash and'],\n",
       " 'model_math_pred': ['',\n",
       "  'the cake, Box 3 contains the fan, Box 4 contains the file and the note, Box',\n",
       "  '0 contains the cash and the ticket',\n",
       "  'the bomb and the boot and the plane',\n",
       "  '         '],\n",
       " 'model_code_pred': ['the drink and the stone',\n",
       "  'the camera and the car and the plane',\n",
       "  'the brain and the guitar, Box 2 contains the medicine and the newspaper, Box 3 contains the',\n",
       "  'the brain and the guitar',\n",
       "  'the medicine and the brain and the guitar'],\n",
       " 'sample_id': [0, 0, 6, 6, 6],\n",
       " 'numops': [2, 2, 2, 2, 4]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 333.52ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95517"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all.to_json(f\"./results/eval/preds_fiveshot_small.jsonl\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
