{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install tokenizers\n",
    "!pip install huggingface_hub\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "\n",
    "    # List all directories in the results folder\n",
    "    all_checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "    # Extract the checkpoint numbers\n",
    "    checkpoint_numbers = [int(d.split(\"-\")[1]) for d in all_checkpoints if d.startswith(\"checkpoint-\")]\n",
    "    # Find the folder name of the latest checkpoint\n",
    "    latest_checkpoint_folder = f\"{checkpoint_dir}/checkpoint-{max(checkpoint_numbers)}\"\n",
    "    \n",
    "    return latest_checkpoint_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(checkpoint_dir):\n",
    "    checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(dataset_dir):\n",
    "    dataset_train_path = f\"{dataset_dir}/train.jsonl\"\n",
    "    #dataset_test_path = f\"{dataset_dir}/test.jsonl\"\n",
    "    dataset_test_path = f\"{dataset_dir}/test_small.jsonl\"\n",
    "    dataset_train = load_dataset('json', data_files=dataset_train_path)['train']\n",
    "    dataset_test = load_dataset('json', data_files=dataset_test_path)['train']\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Given the description after \"Description:\", complete the last sentence with a true statement about the contents\n",
    "of the specified box according to the description.\n",
    "Description: \n",
    "\"\"\"\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for entry in tqdm(dataset):\n",
    "        # tokenize problem\n",
    "        input_data = PROMPT + entry['sentence_masked'][:-15] + \":\"\n",
    "        input_encoding = tokenizer(input_data, truncation=True, padding='max_length', max_length=512, return_attention_mask = True, return_tensors = 'pt')\n",
    "        input_ids.append(input_encoding['input_ids'])\n",
    "        attention_masks.append(input_encoding['attention_mask'])\n",
    "         # tokenize answer\n",
    "        target = entry['masked_content'][13:]\n",
    "        target_encoding = tokenizer(target, truncation=True, padding='max_length', max_length=512, return_attention_mask = True, return_tensors = 'pt')\n",
    "        target_input_ids = target_encoding['input_ids']\n",
    "        target_input_ids[target_input_ids == tokenizer.pad_token_id] = -100\n",
    "        labels.append(target_input_ids)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    tokenized_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "def make_dataloader(tokenized_dataset):\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Shot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_dir):\n",
    "    checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader):\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = 1e-4, eps = 1e-8)\n",
    "\n",
    "    for _ in tqdm(range(5)):\n",
    "\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "\n",
    "          input_ids = batch[0].to(device)\n",
    "          input_mask = batch[1].to(device)\n",
    "          labels = batch[2].to(device)\n",
    "          \n",
    "          model.zero_grad()\n",
    "          outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "          loss = outputs.loss\n",
    "          total_train_loss += loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "          optimizer.step()\n",
    "    \n",
    "    print(f\"Total train loss: {total_train_loss}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_all(model, tokenizer, dataloader):\n",
    "    model_predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # Generate sequences for the batch\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        sequence_ids = model.generate(input_ids, attention_mask=input_mask)\n",
    "        sequences = tokenizer.batch_decode(sequence_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Accumulate generated sequences\n",
    "        model_predictions.extend(sequences)\n",
    "\n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_to_data(model_preds, model_name, dataset):\n",
    "    dataset = dataset.add_column(f\"model_{model_name}_pred\", model_preds)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name=\"base\", zeroshot=True):\n",
    "\n",
    "    # prepare datasets\n",
    "    if (model_name == \"base\"):\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    else:\n",
    "        tokenizer = load_tokenizer(f\"./results/{model_name}/\")\n",
    "    dataset_train, dataset_test = load_datasets(\"./eval/current_use\")\n",
    "    tokenized_dataset_train = tokenize_dataset(dataset_train, tokenizer)\n",
    "    tokenized_dataset_test = tokenize_dataset(dataset_test, tokenizer)\n",
    "    train_loader = make_dataloader(tokenized_dataset_train)\n",
    "    test_loader = make_dataloader(tokenized_dataset_test)\n",
    "\n",
    "    # load and potentially train model\n",
    "    if (model_name == \"base\"):\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "    else:\n",
    "        model = load_model(f\"./results/{model_name}/\")\n",
    "    if not zeroshot:\n",
    "        train_model(model, train_loader)\n",
    "\n",
    "    # make predictions\n",
    "    model_preds = make_prediction_all(model, tokenizer, test_loader)\n",
    "    dataset_test = add_prediction_to_data(model_preds, model_name, dataset_test)\n",
    "    dataset_test.to_json(f\"./results/eval/preds_{model_name}_zeroshot.jsonl\", orient=\"records\")\n",
    "    #dataset_test.to_json(f\"./results/eval/preds_{model_name}.jsonl\", orient=\"records\")\n",
    "    \n",
    "    # garbage collection\n",
    "    del model, tokenizer, tokenized_dataset_train, tokenized_dataset_test, train_loader, test_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model_preds, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Generating train split: 14 examples [00:00, 2800.47 examples/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 2000.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1246.35it/s]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.55s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 4.514426231384277.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:06<00:00,  3.68it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1043.62ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_math, dataset_test_math = evaluate_model(model_name=\"math\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_math_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 286.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1922.89it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.10s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "100%|██████████| 5/5 [00:25<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 4.321694850921631.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.63it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 499.68ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_code1, dataset_test_code1 = evaluate_model(model_name=\"code1\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_code1_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_code1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 285.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1868.82it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "100%|██████████| 5/5 [00:26<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 4.195098876953125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.33it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 499.32ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_code2, dataset_test_code2 = evaluate_model(model_name=\"code2\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_code2_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_code2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 307.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1738.90it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train loss: 2.103578567504883.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:06<00:00,  3.73it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 500.22ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_base, dataset_test_base = evaluate_model(model_name=\"base\", zeroshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentence_masked', 'masked_content', 'sample_id', 'numops', 'model_base_pred'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_all = Dataset.from_dict({\n",
    "    # problem and solution\n",
    "    'sentence_masked': dataset_test_base['sentence_masked'],\n",
    "    'masked_content': dataset_test_base['masked_content'],\n",
    "    # predictions\n",
    "    'model_base_pred': model_preds_base,\n",
    "    'model_math_pred': model_preds_math,\n",
    "    'model_code1_pred': model_preds_code1,\n",
    "    'model_code2_pred': model_preds_code2,\n",
    "    # metadata\n",
    "    'sample_id': dataset_test_base['sample_id'],\n",
    "    'numops': dataset_test_base['numops'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence_masked', 'masked_content', 'model_base_pred', 'model_math_pred', 'model_code1_pred', 'model_code2_pred', 'sample_id', 'numops'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_masked': ['Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 0 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 1 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 2 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 3 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 4 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 5 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 6 contains <extra_id_0> .'],\n",
       " 'masked_content': ['<extra_id_0> the boat and the shoe',\n",
       "  '<extra_id_0> the camera and the car and the plane',\n",
       "  '<extra_id_0> the chemical and the drink and the stone',\n",
       "  '<extra_id_0> the fan',\n",
       "  '<extra_id_0> the cross and the file and the note',\n",
       "  '<extra_id_0> the dish',\n",
       "  '<extra_id_0> the cake and the television'],\n",
       " 'model_base_pred': ['the boat and the shoe and the television',\n",
       "  'the car and the plane',\n",
       "  'the cake',\n",
       "  'the fan',\n",
       "  'the cross and the file and the note',\n",
       "  'the chemical',\n",
       "  'the cake'],\n",
       " 'model_math_pred': ['the cross and the file',\n",
       "  'the chemical and the stone',\n",
       "  'the chemical',\n",
       "  'the fan',\n",
       "  'the cross and the file and the note',\n",
       "  'the cross and the file and the note',\n",
       "  'the chemical'],\n",
       " 'model_code1_pred': ['the cake',\n",
       "  'the dish and the stone',\n",
       "  'the cake',\n",
       "  'the cross and the file and the note',\n",
       "  'the boat and the shoe',\n",
       "  'the cross and the file and the note',\n",
       "  'the cake'],\n",
       " 'model_code2_pred': ['the cross',\n",
       "  'the boat and the shoe and the television',\n",
       "  'the cake',\n",
       "  'the dish and the stone',\n",
       "  'the drink',\n",
       "  'the shoe and the television',\n",
       "  'the dish and the stone'],\n",
       " 'sample_id': [0, 0, 0, 0, 0, 0, 0],\n",
       " 'numops': [1, 0, 3, 0, 0, 2, 2]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 999.83ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88025"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all.to_json(f\"./results/eval/preds_twoshot_small.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 333.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1834.23it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:06<00:00,  3.97it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 999.60ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_math, dataset_test_math = evaluate_model(model_name=\"math\", zeroshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 2333.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1218.88it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.17it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1000.55ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_code1, dataset_test_code1 = evaluate_model(model_name=\"code1\", zeroshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 2333.60it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1226.06it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:06<00:00,  3.97it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 999.12ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_code2, dataset_test_code2 = evaluate_model(model_name=\"code2\", zeroshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 14/14 [00:00<00:00, 2000.21it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1069.83it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [00:06<00:00,  4.07it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 990.16ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds_base, dataset_test_base = evaluate_model(model_name=\"base\", zeroshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_all = Dataset.from_dict({\n",
    "    # problem and solution\n",
    "    'sentence_masked': dataset_test_base['sentence_masked'],\n",
    "    'masked_content': dataset_test_base['masked_content'],\n",
    "    # predictions\n",
    "    'model_base_pred': model_preds_base,\n",
    "    'model_math_pred': model_preds_math,\n",
    "    'model_code1_pred': model_preds_code1,\n",
    "    'model_code2_pred': model_preds_code2,\n",
    "    # metadata\n",
    "    'sample_id': dataset_test_base['sample_id'],\n",
    "    'numops': dataset_test_base['numops'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_masked': ['Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 0 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 1 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 2 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 3 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 4 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 5 contains <extra_id_0> .',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera and the car and the plane, Box 2 contains the cake, Box 3 contains the fan, Box 4 contains the cross and the file and the note, Box 5 contains the chemical, Box 6 contains nothing. Move the cake from Box 2 to Box 6. Put the dish and the stone into Box 5. Move the television from Box 0 to Box 6. Put the drink into Box 2. Move the chemical and the stone from Box 5 to Box 2. Box 6 contains <extra_id_0> .'],\n",
       " 'masked_content': ['<extra_id_0> the boat and the shoe',\n",
       "  '<extra_id_0> the camera and the car and the plane',\n",
       "  '<extra_id_0> the chemical and the drink and the stone',\n",
       "  '<extra_id_0> the fan',\n",
       "  '<extra_id_0> the cross and the file and the note',\n",
       "  '<extra_id_0> the dish',\n",
       "  '<extra_id_0> the cake and the television'],\n",
       " 'model_base_pred': ['Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera',\n",
       "  'Box 0 contains the boat and the shoe and the television, Box 1 contains the camera'],\n",
       " 'model_math_pred': ['The box contains the boat and shoe and the television, Box 1 contains the camera and the car',\n",
       "  'The box contains the boat and shoe and the television, Box 1 contains the camera and the car',\n",
       "  'The box contains the boat and shoe and the television, Box 1 contains the camera and the car',\n",
       "  'The box contains the boat and shoe and the television, Box 1 contains the camera and the car',\n",
       "  'The box contains the boat and shoe and the television, Box 1 contains the camera and the car',\n",
       "  'The box contains the following: Box 0 contains the boat and shoe and box 2 contains the',\n",
       "  'The box contains the following: Box 0 contains the boat and shoe and box 2 contains the'],\n",
       " 'model_code1_pred': ['python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)',\n",
       "  'python def num_boxes(boxes)'],\n",
       " 'model_code2_pred': ['python def num_sequences(se',\n",
       "  'python def num_sequences(se',\n",
       "  'python def num_sequences(se',\n",
       "  'python def num_sequences(se',\n",
       "  'python def num_sequences(se',\n",
       "  'python def num_sequences(se',\n",
       "  'python def num_sequences(se'],\n",
       " 'sample_id': [0, 0, 0, 0, 0, 0, 0],\n",
       " 'numops': [1, 0, 3, 0, 0, 2, 2]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 500.75ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101648"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_all.to_json(f\"./results/eval/preds_zeroshot.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_if_not_already(dataset_path=\"./results/eval/preds_twoshot.jsonl\"):\n",
    "    if \"dataset_test_all\" not in locals():\n",
    "        dataset_test_all = load_dataset('json', data_files=dataset_path)['train']\n",
    "    return dataset_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(labels):\n",
    "    # removes leading masked token\n",
    "    cln_labels = list(map(lambda label: label[13:], labels))\n",
    "    return cln_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_model(labels_combined, preds_combined):\n",
    "\n",
    "    # split labels and preds into list of items\n",
    "    label_items = list(map(lambda label: label.split(\" and \"), labels_combined))\n",
    "    preds_items = list(map(lambda pred: pred.split(\" and \"), preds_combined))\n",
    "\n",
    "    # count predictions\n",
    "    total_preds, total_correct, true_pos, false_pos, false_neg = 0, 0, 0, 0, 0\n",
    "    for i in range(len(label_items)):\n",
    "        labels, preds = label_items[i], preds_items[i]\n",
    "        for label in labels:\n",
    "            if (label in preds):     # correct answer that pred also has\n",
    "                total_correct += 1\n",
    "                true_pos += 1\n",
    "            else:                    # correct answer that pred does not have\n",
    "                false_neg += 1\n",
    "            total_preds += 1         # count the item into total regardless\n",
    "        for pred in preds:\n",
    "            if (pred not in labels): # item that pred has but is not in answer\n",
    "                false_pos += 1\n",
    "\n",
    "    # compute metrics\n",
    "    accuracy = total_correct / total_preds\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    print(f\"precision: {precision}\")\n",
    "    print(f\"recall: {recall}\")\n",
    "    print(f\"f1 score: {f1}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model=\"base\", dataset_path=\"./results/eval/preds_twoshot.jsonl\"):\n",
    "    dataset_test_all = load_dataset_if_not_already(dataset_path=dataset_path)\n",
    "    cln_labels = clean_labels(dataset_test_all['masked_content'])\n",
    "    metrics = compute_metrics_for_model(cln_labels, dataset_test_all[f\"model_{model}_pred\"])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4057142857142857\n",
      "precision: 0.3879781420765027\n",
      "recall: 0.4057142857142857\n",
      "f1 score: 0.19832402234636873\n"
     ]
    }
   ],
   "source": [
    "metrics_base = compute_metrics(\"base\", dataset_path=\"./results/eval/preds_twoshot_small.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.08\n",
      "precision: 0.08433734939759036\n",
      "recall: 0.08\n",
      "f1 score: 0.04105571847507331\n"
     ]
    }
   ],
   "source": [
    "metrics_math = compute_metrics(\"math\", dataset_path=\"./results/eval/preds_twoshot_small.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.10857142857142857\n",
      "precision: 0.12179487179487179\n",
      "recall: 0.10857142857142857\n",
      "f1 score: 0.05740181268882176\n"
     ]
    }
   ],
   "source": [
    "metrics_code1 = compute_metrics(\"code1\", dataset_path=\"./results/eval/preds_twoshot_small.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.10857142857142857\n",
      "precision: 0.10106382978723404\n",
      "recall: 0.10857142857142857\n",
      "f1 score: 0.05234159779614325\n"
     ]
    }
   ],
   "source": [
    "metrics_code2 = compute_metrics(\"code2\", dataset_path=\"./results/eval/preds_twoshot_small.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
