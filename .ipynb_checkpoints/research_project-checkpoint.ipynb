{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n",
    "pip install datasets\n",
    "pip install transformers[torch]\n",
    "pip install tokenizers\n",
    "pip install evaluate\n",
    "pip install rouge_score\n",
    "pip install sentencepiece\n",
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sentencepiece\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0a323b-428f-4ca2-88d9-3bd4145031ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samch\\micromambaenv\\envs\\cs109a\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f8f8f-24af-4ada-addd-a65f1cda6338",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f352-aa2e-430f-b835-239807af143e",
   "metadata": {},
   "source": [
    "### Create math dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64c35803-3c0b-44c0-900b-a5c70864de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_math(folder_path):\n",
    "    data = {\"id\": [], \"question\": [], \"level\": [], \"type\": [], \"answer\": []}\n",
    "\n",
    "    subject_dictionary = {\n",
    "        \"algebra\": 1,\n",
    "        \"counting_and_probability\": 2,\n",
    "        \"geometry\": 3,\n",
    "        \"intermediate_algebra\": 4,\n",
    "        \"number_theory\": 5,\n",
    "        \"prealgebra\": 6,\n",
    "        \"precalculus\": 7\n",
    "    }\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                problem = json_data.get(\"problem\", \"\")\n",
    "                level = json_data.get(\"level\", \"\")\n",
    "                type_ = json_data.get(\"type\", \"\")\n",
    "                solution = json_data.get(\"solution\", \"\")\n",
    "\n",
    "                # Generate id from subject code and file name\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                subject_code = subject_dictionary.get(folder_name, 0)  # Default to 0 if not found\n",
    "                id_ = f\"{subject_code}_{file_name}\"\n",
    "\n",
    "                data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(problem)\n",
    "                data[\"level\"].append(level)\n",
    "                data[\"type\"].append(type_)\n",
    "                data[\"answer\"].append(solution)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee114fa9-509b-47d1-9039-ca0679324080",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train = convert_folder_to_datasetdict_math(\"math/train/\")\n",
    "math_test = convert_folder_to_datasetdict_math(\"math/test/\")\n",
    "\n",
    "math_dict = DatasetDict({\n",
    "    'train': math_train,\n",
    "    'test': math_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9ab3a02-d628-48df-8434-201d28128ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1_1000',\n",
       " 'question': 'What is the degree of the polynomial $(4 +5x^3 +100 +2\\\\pi x^4 + \\\\sqrt{10}x^4 +9)$?',\n",
       " 'level': 'Level 3',\n",
       " 'type': 'Algebra',\n",
       " 'answer': \"This polynomial is not written in standard form.  However, we don't need to write it in standard form, nor do we need to pay attention to the coefficients.  We just look for the exponents on $x$.  We have an $x^4$ term and no other term of higher degree, so $\\\\boxed{4}$ is the degree of the polynomial.\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dict[\"train\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e5ce-60d4-4910-a253-43809da4be7f",
   "metadata": {},
   "source": [
    "### Create code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5b4e88-cfee-412d-bc7f-b4ba799b29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    data = {\"id\": [], \"question\": [], \"type\": [], \"answer\": []}\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            id_ = f\"{file_name}\"\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                content = f.readlines()\n",
    "\n",
    "                # Extract comments as questions\n",
    "                comments = [line.strip()[2:] for line in content if line.strip().startswith('//')]\n",
    "                question = '\\n'.join(comments)\n",
    "                # Extract non-commented lines as answers\n",
    "                code_lines = [line.strip() for line in content if not line.strip().startswith('//')]\n",
    "                answer = '\\n'.join(code_lines)\n",
    "\n",
    "                data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(question)\n",
    "                data[\"type\"].append(folder_name)\n",
    "                data[\"answer\"].append(answer)\n",
    "                \n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a74975-7dca-416b-a4ba-3e077498efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = convert_folder_to_datasetdict_code(\"code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beab82c5-4890-4a4c-9b99-203e2ae145ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dict = code_dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f4e043-85c1-4b18-a5a6-c84fc98d2c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'type', 'answer'],\n",
       "        num_rows: 136\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'type', 'answer'],\n",
       "        num_rows: 59\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f8d63f-ee26-476e-90c1-c2b7c7b69b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'GeneralizedAbbreviation',\n",
       " 'question': ' Write a function to generate the generalized abbreviations of a word.\\n Example:\\n Given word = \"word\", return the following list (order does not matter):\\n [\"word\", \"1ord\", \"w1rd\", \"wo1d\", \"wor1\", \"2rd\", \"w2d\", \"wo2\", \"1o1d\", \"1or1\", \"w1r1\", \"1o2\", \"2r1\", \"3d\", \"w3\", \"4\"]',\n",
       " 'type': 'backtracking',\n",
       " 'answer': '\\n\\npublic class GeneralizedAbbreviation {\\npublic List<String> generateAbbreviations(String word) {\\nList<String> result = new ArrayList<String>();\\n\\nbacktrack(result, word, 0, \"\", 0);\\n\\nreturn result;\\n}\\n\\nvoid backtrack(List result, String word, int position, String current, int count) {\\nif(position == word.length()) {\\nif(count > 0) {\\ncurrent += count;\\n}\\n\\nresult.add(current);\\n} else {\\nbacktrack(result, word, position + 1, current, count + 1);\\nbacktrack(result, word, position + 1, current + (count > 0 ? count : \"\") + word.charAt(position), 0);\\n}\\n}\\n}'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977fd4e-3fa5-4eff-b855-3af20d8d5288",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c905e62b-8fa0-40a1-8441-041309ba0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5355b55-7f8e-45a2-b6d1-91357968dedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.046640872955322266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 7500,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c9acd0464647d084e4be7a7474e365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027621984481811523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 5004,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be32992166c4bbf885e679f5fe0a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03834056854248047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 136,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fde4f9b1e1b498983335c95453d3c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03414773941040039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 59,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4ab5c1bd544a78b9c1cbc7fde89de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_math = math_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code = code_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efdd0266-9885-42ed-8a12-0de5a8cd1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd4453a-5b62-4c43-be98-911e0d460396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb6b0-9938-4584-a913-d523357eb3c6",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bec7ac9-d105-4fef-8c5e-131c45951ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79eb7ce-d4d2-433b-8deb-bb660067cf59",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "035495c8-f1ed-49c8-8cee-713de3910148",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_math = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset_math[\"train\"], \n",
    "   eval_dataset=tokenized_dataset_math[\"test\"],   \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a4338-78e9-4357-9b4b-522a9e37ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_math.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41385fbe-3c12-4db7-8e08-3484cecbcd9c",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2688bf-59de-448b-be88-84cec4e22588",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_code = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset_code[\"train\"],    \n",
    "   eval_dataset=tokenized_dataset_code[\"test\"],    \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f303a-81ea-4ddb-b787-e3c5853976de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_code.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
