{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: datasets in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.1.1+cu118)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\yezix\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.7.22)\n",
      "Requirement already satisfied: evaluate in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: wandb in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\yezix\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.36.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (69.0.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (4.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\yezix\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install tokenizers\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install sentencepiece\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sentencepiece\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f60ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a0a323b-428f-4ca2-88d9-3bd4145031ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samch\\micromambaenv\\envs\\cs109a\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model_code1 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model_code2 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "data_collator_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_math)\n",
    "data_collator_code1 = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_code1)\n",
    "data_collator_code2 = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_code2)\n",
    "data_collator_general = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f8f8f-24af-4ada-addd-a65f1cda6338",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e5ce-60d4-4910-a253-43809da4be7f",
   "metadata": {},
   "source": [
    "### Create code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5cb0b6-228d-45a0-9e72-4dba309f94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "    #df.rename(columns={'code_with_problem': 'question', 'code_only': 'answer'}, inplace=True)\n",
    "    #dataset = Dataset.from_pandas(df[['question', 'answer']])\n",
    "    dataset = Dataset.from_pandas(df[['code_with_problem', 'code_with_data', 'code_only']])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7801c5e5-ca37-4247-b821-0e636bde1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = convert_folder_to_datasetdict_code('data/code/leetcode-solutions.json')\n",
    "train_dataset, test_dataset = train_test_split(code_dataset, test_size=0.2)\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae320bc5-d87c-4a75-8e6c-d43ccc27db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03217e55-403e-49f4-809e-98c00df21937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset_modified = code_dict['train'].rename_column('code_with_problem', 'question')\n",
    "train_dataset_modified = train_dataset_modified.rename_column('code_only', 'answer')\n",
    "train_dataset_modified = train_dataset_modified.remove_columns('code_with_data')\n",
    "\n",
    "test_dataset_modified = code_dict['test'].rename_column('code_with_problem', 'question')\n",
    "test_dataset_modified = test_dataset_modified.rename_column('code_only', 'answer')\n",
    "test_dataset_modified = test_dataset_modified.remove_columns('code_with_data')\n",
    "\n",
    "code_dict1 = DatasetDict({\n",
    "    'train': train_dataset_modified,\n",
    "    'test': test_dataset_modified\n",
    "})\n",
    "\n",
    "print(code_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead256d6-e5f7-465e-bb32-9b473fdadc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset_modified = code_dict['train'].rename_column('code_with_data', 'question')\n",
    "train_dataset_modified = train_dataset_modified.rename_column('code_only', 'answer')\n",
    "train_dataset_modified = train_dataset_modified.remove_columns('code_with_problem')\n",
    "\n",
    "test_dataset_modified = code_dict['test'].rename_column('code_with_data', 'question')\n",
    "test_dataset_modified = test_dataset_modified.rename_column('code_only', 'answer')\n",
    "test_dataset_modified = test_dataset_modified.remove_columns('code_with_problem')\n",
    "\n",
    "code_dict2 = DatasetDict({\n",
    "    'train': train_dataset_modified,\n",
    "    'test': test_dataset_modified\n",
    "})\n",
    "\n",
    "print(code_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1fe247-c841-4170-94c8-d97a89877b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = code_dict['train'].num_rows\n",
    "n_test = code_dict['test'].num_rows\n",
    "n_total = n_train + n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a43a410-aa83-46d1-a752-6e3b0387a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# distinct-numbers-in-each-subarray\\n# Distinct Numbers in Each Subarray\\n# Medium\\n# Given an integer array `nums` and an integer `k`, you are asked to construct the array `ans` of size `n-k+1` where `ans[i]` is the number of **distinct** numbers in the subarray `nums[i:i+k-1] = [nums[i], nums[i+1], ..., nums[i+k-1]]`.\\n\\nReturn _the array_ `ans`.\\n\\n**Example 1:**\\n\\n**Input:** nums = \\\\[1,2,3,2,2,1,3\\\\], k = 3\\n**Output:** \\\\[3,2,2,2,3\\\\]\\n**Explanation:** The number of distinct elements in each subarray goes as follows:\\n- nums\\\\[0:2\\\\] = \\\\[1,2,3\\\\] so ans\\\\[0\\\\] = 3\\n- nums\\\\[1:3\\\\] = \\\\[2,3,2\\\\] so ans\\\\[1\\\\] = 2\\n- nums\\\\[2:4\\\\] = \\\\[3,2,2\\\\] so ans\\\\[2\\\\] = 2\\n- nums\\\\[3:5\\\\] = \\\\[2,2,1\\\\] so ans\\\\[3\\\\] = 2\\n- nums\\\\[4:6\\\\] = \\\\[2,1,3\\\\] so ans\\\\[4\\\\] = 3\\n\\n**Example 2:**\\n\\n**Input:** nums = \\\\[1,1,1,1,2,3,4\\\\], k = 4\\n**Output:** \\\\[1,2,3,4\\\\]\\n**Explanation:** The number of distinct elements in each subarray goes as follows:\\n- nums\\\\[0:3\\\\] = \\\\[1,1,1,1\\\\] so ans\\\\[0\\\\] = 1\\n- nums\\\\[1:4\\\\] = \\\\[1,1,1,2\\\\] so ans\\\\[1\\\\] = 2\\n- nums\\\\[2:5\\\\] = \\\\[1,1,2,3\\\\] so ans\\\\[2\\\\] = 3\\n- nums\\\\[3:6\\\\] = \\\\[1,2,3,4\\\\] so ans\\\\[3\\\\] = 4\\n\\n**Constraints:**\\n\\n*   `1 <= k <= nums.length <= 105`\\n*   `1 <= nums[i] <= 105`\\n```python\\ndef distinct_numbers_in_subarrays(nums, k):\\n    counts = {}\\n    ans = []\\n    for i, num in enumerate(nums):\\n        counts[num] = counts.get(num, 0) + 1\\n        if i >= k:\\n            counts[nums[i - k]] -= 1\\n            if counts[nums[i - k]] == 0:\\n                del counts[nums[i - k]]\\n        if i >= k - 1:\\n            ans.append(len(counts))\\n    return ans\\n```\\n\\n# We use a sliding window approach to keep track of a subarray of the size k in the given array. To find the distinct elements in the subarray, we maintain a hash map named `counts` to store the frequency of each element in the subarray. We iterate through the given array and perform the following steps for each element:\\n\\n1. Increment the frequency count of the element in the `counts` hash map.\\n2. If the index `i` is larger or equal to `k`, we know the subarray of size k has moved to the next index. We decrement the frequency count of the element at `nums[i - k]`. If the frequency count becomes zero, we remove this element from the hash map.\\n3. If the index `i` is larger or equal to `k - 1`, we have a valid subarray of size k. In this case, we append the length of the hash map (which represents the number of distinct elements) to the `ans` array.\\n\\nAt the end, we return the `ans` array.\\n',\n",
       " 'answer': '```python\\ndef distinct_numbers_in_subarrays(nums, k):\\n    counts = {}\\n    ans = []\\n    for i, num in enumerate(nums):\\n        counts[num] = counts.get(num, 0) + 1\\n        if i >= k:\\n            counts[nums[i - k]] -= 1\\n            if counts[nums[i - k]] == 0:\\n                del counts[nums[i - k]]\\n        if i >= k - 1:\\n            ans.append(len(counts))\\n    return ans\\n```\\n\\n'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict1[\"train\"][0]\n",
    "code_dict2[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633e05f-5423-4e20-8c09-1346d19ef328",
   "metadata": {},
   "source": [
    "### Create general knowledge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b8c412-f8a8-4fa8-ab4a-5bea2ae894a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'data/general/general.csv'\n",
    "general = pd.read_csv(csv_file_path)\n",
    "general = general.sample(n=n_total, random_state=42)\n",
    "general['id'] = range(len(general))\n",
    "train_sample, test_sample = train_test_split(general, test_size=0.2, random_state=42)\n",
    "train_sample.reset_index(drop=True, inplace=True)\n",
    "test_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_sample[['question', 'answer']])\n",
    "test_dataset = Dataset.from_pandas(test_sample[['question', 'answer']])\n",
    "\n",
    "general_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "print(general_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57afb3e1-c17b-4ebe-846b-d82948014f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Add 1 letter to \"Iowa\" to get the name of this tribe who lived south of the Iowa',\n",
       " 'answer': 'Kiowa'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f352-aa2e-430f-b835-239807af143e",
   "metadata": {},
   "source": [
    "### Create math dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64c35803-3c0b-44c0-900b-a5c70864de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_math(folder_path):\n",
    "    #data = {\"id\": [], \"question\": [], \"level\": [], \"type\": [], \"answer\": []}\n",
    "    data = {\"question\": [], \"answer\": []}\n",
    "\n",
    "    subject_dictionary = {\n",
    "        \"algebra\": 1,\n",
    "        \"counting_and_probability\": 2,\n",
    "        \"geometry\": 3,\n",
    "        \"intermediate_algebra\": 4,\n",
    "        \"number_theory\": 5,\n",
    "        \"prealgebra\": 6,\n",
    "        \"precalculus\": 7\n",
    "    }\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                problem = json_data.get(\"problem\", \"\")\n",
    "                level = json_data.get(\"level\", \"\")\n",
    "                type_ = json_data.get(\"type\", \"\")\n",
    "                solution = json_data.get(\"solution\", \"\")\n",
    "\n",
    "                # Generate id from subject code and file name\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                subject_code = subject_dictionary.get(folder_name, 0)  # Default to 0 if not found\n",
    "                id_ = f\"{subject_code}_{file_name}\"\n",
    "\n",
    "                #data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(problem)\n",
    "                #data[\"level\"].append(level)\n",
    "                #data[\"type\"].append(type_)\n",
    "                data[\"answer\"].append(solution)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee114fa9-509b-47d1-9039-ca0679324080",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train = convert_folder_to_datasetdict_math(\"data/math/train/\")\n",
    "math_test = convert_folder_to_datasetdict_math(\"data/math/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "820af25a-2806-4cbe-be23-107fc02f632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "math_train = math_train.shuffle(seed=42)\n",
    "math_train = math_train.select(range(n_train))\n",
    "\n",
    "math_test = math_test.shuffle(seed=42)\n",
    "math_test = math_test.select(range(n_test))\n",
    "\n",
    "math_dict = DatasetDict({\n",
    "    'train': math_train,\n",
    "    'test': math_test\n",
    "})\n",
    "\n",
    "print(math_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93a46830-2716-495d-a669-87c046816932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the number of units in the distance between $(2,5)$ and $(-6,-1)$?',\n",
       " 'answer': 'We use the distance formula: $\\\\sqrt{(-6 - 2)^2 + (-1 - 5)^2},$ so then we find that $\\\\sqrt{64 + 36} = \\\\boxed{10}$.\\n\\n- OR -\\n\\nWe note that the points $(2, 5)$, $(-6, -1)$, and $(2, -1)$ form a right triangle with legs of length 6 and 8. This is a Pythagorean triple, so the length of the hypotenuse must be $\\\\boxed{10}$.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977fd4e-3fa5-4eff-b855-3af20d8d5288",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c905e62b-8fa0-40a1-8441-041309ba0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5355b55-7f8e-45a2-b6d1-91357968dedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03151869773864746,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1887,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ce129d3a0648f6aac45a1b188c6e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015758037567138672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 472,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a96bec1544fc09a28631a61a22773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01020669937133789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1887,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2cb5cfb43c4e0ea155f34f79581a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031876564025878906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 472,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4b0c492c0a45849bad13ad1a92a922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031385183334350586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1887,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45c60e0050b45e4b76c488cc261ff03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03132987022399902,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 472,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6d8b98f5874597acb752830e8aceba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016073942184448242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1887,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6cdd50a04e4a0397b4742afaec7805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03125619888305664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 472,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c161f8c4f648e395afa84a7e1bee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_code1 = code_dict1.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code2 = code_dict2.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general = general_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_math = math_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efdd0266-9885-42ed-8a12-0de5a8cd1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cd4453a-5b62-4c43-be98-911e0d460396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb6b0-9938-4584-a913-d523357eb3c6",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bec7ac9-d105-4fef-8c5e-131c45951ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "OVERWRITE_OUTPUT_DIR = True\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "\n",
    "# Set up training arguments\n",
    "training_args_math = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results/math\",\n",
    "   overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "   save_strategy=\"epoch\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "training_args_code1 = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results/code1\",\n",
    "   overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "   save_strategy=\"epoch\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "training_args_code2 = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results/code2\",\n",
    "   overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "   save_strategy=\"epoch\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "training_args_general = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results/base\",\n",
    "   overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "   save_strategy=\"epoch\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79eb7ce-d4d2-433b-8deb-bb660067cf59",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "035495c8-f1ed-49c8-8cee-713de3910148",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_math = Seq2SeqTrainer(\n",
    "   model=model_math,\n",
    "   args=training_args_math,\n",
    "   train_dataset=tokenized_dataset_math[\"train\"], \n",
    "   eval_dataset=tokenized_dataset_math[\"test\"],   \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_math,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5a4338-78e9-4357-9b4b-522a9e37ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231123_151548-i0svx75r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/i0svx75r' target=\"_blank\">apricot-lion-6</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/i0svx75r' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/i0svx75r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 501/2814 [00:59<04:35,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0484, 'learning_rate': 0.0002466950959488273, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 938/2814 [01:49<03:37,  8.64it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 938/2814 [02:47<03:37,  8.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 58.1889, 'eval_samples_per_second': 85.927, 'eval_steps_per_second': 21.482, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1002/2814 [02:57<03:28,  8.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00019339019189765458, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1501/2814 [03:56<02:49,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00014008528784648186, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1876/2814 [04:41<01:53,  8.27it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 67%|██████▋   | 1876/2814 [05:40<01:53,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 59.3238, 'eval_samples_per_second': 84.283, 'eval_steps_per_second': 21.071, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 2002/2814 [05:57<01:31,  8.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 8.678038379530917e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2502/2814 [06:54<00:35,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 3.347547974413646e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2814/2814 [07:31<00:00,  8.96it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 2814/2814 [08:32<00:00,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 61.2103, 'eval_samples_per_second': 81.686, 'eval_steps_per_second': 20.421, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 2814/2814 [08:36<00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 518.6125, 'train_samples_per_second': 43.385, 'train_steps_per_second': 5.426, 'train_loss': 0.00859833234957453, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2814, training_loss=0.00859833234957453, metrics={'train_runtime': 518.6125, 'train_samples_per_second': 43.385, 'train_steps_per_second': 5.426, 'train_loss': 0.00859833234957453, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_math.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62cb8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_math\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41385fbe-3c12-4db7-8e08-3484cecbcd9c",
   "metadata": {},
   "source": [
    "### Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff2688bf-59de-448b-be88-84cec4e22588",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_code1 = Seq2SeqTrainer(\n",
    "   model=model_code1,\n",
    "   args=training_args_code1,\n",
    "   train_dataset=tokenized_dataset_code1[\"train\"],    \n",
    "   eval_dataset=tokenized_dataset_code1[\"test\"],    \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_code1,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f0f303a-81ea-4ddb-b787-e3c5853976de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16/48 [03:28<07:55, 14.87s/it]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 16/48 [03:47<07:55, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.496579885482788, 'eval_rouge1': 0.03383376969610924, 'eval_rouge2': 0.0016460905349794238, 'eval_rougeL': 0.028476556253146653, 'eval_rougeLsum': 0.029104003651678312, 'eval_runtime': 18.5042, 'eval_samples_per_second': 2.918, 'eval_steps_per_second': 0.757, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 32/48 [07:24<02:47, 10.45s/it]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 32/48 [07:42<02:47, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.085508108139038, 'eval_rouge1': 0.14193502014963366, 'eval_rouge2': 0.05224203937530196, 'eval_rougeL': 0.1311783087726054, 'eval_rougeLsum': 0.1319044507434023, 'eval_runtime': 18.0557, 'eval_samples_per_second': 2.991, 'eval_steps_per_second': 0.775, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [10:58<00:00, 12.77s/it]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 48/48 [11:16<00:00, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9809935092926025, 'eval_rouge1': 0.1505172341955189, 'eval_rouge2': 0.05507006806796853, 'eval_rougeL': 0.1411070353465603, 'eval_rougeLsum': 0.1417876510239457, 'eval_runtime': 18.1912, 'eval_samples_per_second': 2.968, 'eval_steps_per_second': 0.77, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 48/48 [11:22<00:00, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 682.7185, 'train_samples_per_second': 0.554, 'train_steps_per_second': 0.07, 'train_loss': 2.8900632858276367, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=2.8900632858276367, metrics={'train_runtime': 682.7185, 'train_samples_per_second': 0.554, 'train_steps_per_second': 0.07, 'train_loss': 2.8900632858276367, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_code1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4424493",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_code1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4f37b-4fe4-4f72-a94f-f2a152470428",
   "metadata": {},
   "source": [
    "### Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fa8e107-32a6-4c9f-a0a8-a000484ddedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_code2 = Seq2SeqTrainer(\n",
    "   model=model_code2,\n",
    "   args=training_args_code2,\n",
    "   train_dataset=tokenized_dataset_code2[\"train\"],    \n",
    "   eval_dataset=tokenized_dataset_code2[\"test\"],    \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_code2,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59524f6c-a08e-4c20-b387-ab365f4068d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_code2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905175a5-ef86-4832-a1d8-d9ee80108d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_code2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a264ec7-bbb0-4142-b6f8-7fcd12b2e7b1",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f3883a6-fd6b-463b-8674-d08f81261ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_general = Seq2SeqTrainer(\n",
    "   model=model_general,\n",
    "   args=training_args_general,\n",
    "   train_dataset=tokenized_dataset_general[\"train\"],    \n",
    "   eval_dataset=tokenized_dataset_general[\"test\"],    \n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_general,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc2673-534a-4555-9159-a220f860e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_general.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae22c6-e4dd-4375-a831-f554d61ec308",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_general\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
