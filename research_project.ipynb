{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: datasets in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: pandas in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (0.0.0)\n",
      "Requirement already satisfied: packaging in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: filelock in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Requirement already satisfied: tokenizers in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: evaluate in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: pandas in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (0.12.1)\n",
      "Requirement already satisfied: packaging in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: multiprocess in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: dill in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (1.24.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (0.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: rouge_score in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from rouge_score) (1.24.2)\n",
      "Requirement already satisfied: absl-py in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk->rouge_score) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: sentencepiece in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: huggingface_hub in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: filelock in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: wandb in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: setproctitle in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: setuptools in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (67.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (1.38.0)\n",
      "Requirement already satisfied: PyYAML in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install tokenizers\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install sentencepiece\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f60ea4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0a323b-428f-4ca2-88d9-3bd4145031ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f8f8f-24af-4ada-addd-a65f1cda6338",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e5ce-60d4-4910-a253-43809da4be7f",
   "metadata": {},
   "source": [
    "### Create code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb437f7-ae45-41ad-9029-e3b87d133e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_before_example(text):\n",
    "    return text.split(\"**Example 1:**\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5cb0b6-228d-45a0-9e72-4dba309f94c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "    dataset = Dataset.from_pandas(df[['code_with_problem', 'code_only']])\n",
    "    dataset['question'] = dataset['code_with_problem'].apply(extract_text_before_example)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c13de88-ac92-411a-b2e4-f5fbc031f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    # Extract text before \"Example 1\" for the 'code_with_problem' column\n",
    "    df['question'] = df['code_with_problem'].apply(extract_text_before_example)\n",
    "\n",
    "    # Rename columns and create the dataset\n",
    "    df.rename(columns={'code_only': 'answer'}, inplace=True)\n",
    "    dataset = Dataset.from_pandas(df[['question', 'answer']])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7801c5e5-ca37-4247-b821-0e636bde1f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "code_dataset = convert_folder_to_datasetdict_code('data/code/leetcode-solutions.json')\n",
    "train_dataset, test_dataset = train_test_split(code_dataset, test_size=0.2)\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03217e55-403e-49f4-809e-98c00df21937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1fe247-c841-4170-94c8-d97a89877b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = code_dict['train'].num_rows\n",
    "n_test = code_dict['test'].num_rows\n",
    "n_total = n_train + n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a43a410-aa83-46d1-a752-6e3b0387a9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are given an array `points` where `points[i] = [xi, yi]` represents a point on an **X-Y** plane.\\n\\n**Straight lines** are going to be added to the **X-Y** plane, such that every point is covered by at **least** one line.\\n\\nReturn _the **minimum** number of **straight lines** needed to cover all the points_.',\n",
       " 'answer': '```python\\nfrom math import gcd\\nfrom itertools import combinations\\n\\ndef minStraightLines(points):\\n    lines = set()\\n    for p1, p2 in combinations(points, 2):\\n        dy, dx = p2[1] - p1[1], p2[0] - p1[0]\\n        g = gcd(dx, dy)\\n        lines.add((dy // g, dx // g))\\n    return len(lines)\\n```\\n\\n\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633e05f-5423-4e20-8c09-1346d19ef328",
   "metadata": {},
   "source": [
    "### Create general knowledge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b8c412-f8a8-4fa8-ab4a-5bea2ae894a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n",
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'data/general/general.csv'\n",
    "general = pd.read_csv(csv_file_path)\n",
    "general = general.sample(n=n_total, random_state=42)\n",
    "general['id'] = range(len(general))\n",
    "train_sample, test_sample = train_test_split(general, test_size=0.2, random_state=42)\n",
    "train_sample.reset_index(drop=True, inplace=True)\n",
    "test_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_sample[['question', 'answer']])\n",
    "test_dataset = Dataset.from_pandas(test_sample[['question', 'answer']])\n",
    "\n",
    "general_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "print(general_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57afb3e1-c17b-4ebe-846b-d82948014f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Add 1 letter to \"Iowa\" to get the name of this tribe who lived south of the Iowa',\n",
       " 'answer': 'Kiowa'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f352-aa2e-430f-b835-239807af143e",
   "metadata": {},
   "source": [
    "### Create math dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c35803-3c0b-44c0-900b-a5c70864de2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_math(folder_path):\n",
    "    #data = {\"id\": [], \"question\": [], \"level\": [], \"type\": [], \"answer\": []}\n",
    "    data = {\"question\": [], \"answer\": []}\n",
    "\n",
    "    subject_dictionary = {\n",
    "        \"algebra\": 1,\n",
    "        \"counting_and_probability\": 2,\n",
    "        \"geometry\": 3,\n",
    "        \"intermediate_algebra\": 4,\n",
    "        \"number_theory\": 5,\n",
    "        \"prealgebra\": 6,\n",
    "        \"precalculus\": 7\n",
    "    }\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                problem = json_data.get(\"problem\", \"\")\n",
    "                level = json_data.get(\"level\", \"\")\n",
    "                type_ = json_data.get(\"type\", \"\")\n",
    "                solution = json_data.get(\"solution\", \"\")\n",
    "\n",
    "                # Generate id from subject code and file name\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                subject_code = subject_dictionary.get(folder_name, 0)  # Default to 0 if not found\n",
    "                id_ = f\"{subject_code}_{file_name}\"\n",
    "\n",
    "                #data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(problem)\n",
    "                #data[\"level\"].append(level)\n",
    "                #data[\"type\"].append(type_)\n",
    "                data[\"answer\"].append(solution)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee114fa9-509b-47d1-9039-ca0679324080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "math_train = convert_folder_to_datasetdict_math(\"data/math/train/\")\n",
    "math_test = convert_folder_to_datasetdict_math(\"data/math/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "820af25a-2806-4cbe-be23-107fc02f632f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "math_train = math_train.shuffle(seed=42)\n",
    "math_train = math_train.select(range(n_train))\n",
    "\n",
    "math_test = math_test.shuffle(seed=42)\n",
    "math_test = math_test.select(range(n_test))\n",
    "\n",
    "math_dict = DatasetDict({\n",
    "    'train': math_train,\n",
    "    'test': math_test\n",
    "})\n",
    "\n",
    "print(math_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a46830-2716-495d-a669-87c046816932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Three positive integers $a$, $b$, and $c$ satisfy $a\\\\cdot b\\\\cdot c=8!$ and $a<b<c$.  What is the smallest possible value of $c-a$?',\n",
       " 'answer': 'Our goal is to divide the factors of 8! into three groups in such a way that the products of the factors in each group are as close together as possible.  Write $8!$ as $8\\\\cdot 7 \\\\cdot 6 \\\\cdot 5\\\\cdot 4\\\\cdot 3\\\\cdot 2$.  Observe that $30^3<8!<40^3$, so the cube root of $8!$ is between $30$ and $40$.  With this in mind, we group $7$ and $5$ to make one factor of $35$.  We can also make a factor of $36$ by using $6$ along with $3$ and $2$.  This leaves $8$ and $4$ which multiply to give $32$.  The assignment $(a,b,c)=(32,35,36)$ has the minimum value of $c-a$, since $31$, $33$, $34$, $37$, $38$, and $39$ contain prime factors not present in $8!$.  Therefore, the minimum value of $c-a$ is $\\\\boxed{4}$.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79018d-53d9-4042-98d7-3f715aea4ca9",
   "metadata": {},
   "source": [
    "### Create 50% samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e4c8b9-adfa-4f8a-a7ad-c0a7a3504f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "math_train_sample = math_dict['train'].shuffle(seed=42).select([i for i in range(math_dict['train'].num_rows // 2)])\n",
    "math_test_sample = math_dict['test'].shuffle(seed=42).select([i for i in range(math_dict['test'].num_rows // 2)])\n",
    "\n",
    "general_train_sample = general_dict['train'].shuffle(seed=42).select([i for i in range(general_dict['train'].num_rows // 2+1)])\n",
    "general_test_sample = general_dict['test'].shuffle(seed=42).select([i for i in range(general_dict['test'].num_rows // 2)])\n",
    "\n",
    "code_train_sample = code_dict['train'].shuffle(seed=42).select([i for i in range(code_dict['train'].num_rows // 2)])\n",
    "code_test_sample = code_dict['test'].shuffle(seed=42).select([i for i in range(code_dict['test'].num_rows // 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74577ff-b47a-4c4a-bb30-779c72336122",
   "metadata": {},
   "source": [
    "### Create 50% general 50% math dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5cbcd89-e9ff-4f64-b38f-8b82f96a31eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "general_math_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': math_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': math_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': math_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': math_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "\n",
    "general_math_dict = DatasetDict({\n",
    "    'train': general_math_dict['train'].shuffle(seed=42),\n",
    "    'test': general_math_dict['test'].shuffle(seed=42)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882a1fc2-3d94-4836-8a1d-c2eea04fd356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A 4-inch by 6-inch picture is enlarged for framing  by tripling its dimensions.  A 2-inch-wide border  is then placed around each side of the enlarged  picture, as shown.  Thin metal framing is sold only  in increments of one foot.  What is the minimum  number of linear feet of framing that must be  purchased to go around the perimeter of the border?\\n\\n[asy]\\n\\ndraw((0,0)--(14,0)--(14,20)--(0,20)--cycle,linewidth(2));\\n\\ndraw((4,4)--(10,4)--(10,16)--(4,16)--cycle);\\n\\nlabel(\"border\",(7,17),N);\\n\\nlabel(\"picture\",(7,8),N);\\n\\nlabel(\"frame\",(14,5),E);\\n\\ndraw((17.5,7.5)--(14.5,7.5),Arrow);\\ndraw((10.5,7.5)--(13.5,7.5),Arrow);\\n\\n[/asy]',\n",
       " 'answer': 'After the picture is enlarged by tripling its dimensions, the dimensions become $12\\\\times18$. After the border is added, the dimensions of the picture increase to $16\\\\times22$ (since each side has a 2-inch border). The perimeter is $16+16+22+22=76$ inches. Since $76/12=6\\\\frac{1}{3}$, we need $\\\\boxed{7}$ feet of framing to go around the entire picture.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b6262-6ee4-43e1-bd63-564fc5817642",
   "metadata": {},
   "source": [
    "### Create 50% general 50% code dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6fb1f1-c1f0-4860-9f28-e0229cde7487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "general_code_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': code_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': code_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': code_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': code_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "general_code_dict = DatasetDict({\n",
    "    'train': general_code_dict['train'].shuffle(seed=42),\n",
    "    'test': general_code_dict['test'].shuffle(seed=42)\n",
    "})\n",
    "\n",
    "print(general_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc493dae-6232-483a-98b7-2413a0148097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are given an array `nums`. You can rotate it by a non-negative integer `k` so that the array becomes `[nums[k], nums[k + 1], ... nums[nums.length - 1], nums[0], nums[1], ..., nums[k-1]]`. Afterward, any entries that are less than or equal to their index are worth one point.\\n\\n*   For example, if we have `nums = [2,4,1,3,0]`, and we rotate by `k = 2`, it becomes `[1,3,0,2,4]`. This is worth `3` points because `1 > 0` \\\\[no points\\\\], `3 > 1` \\\\[no points\\\\], `0 <= 2` \\\\[one point\\\\], `2 <= 3` \\\\[one point\\\\], `4 <= 4` \\\\[one point\\\\].\\n\\nReturn _the rotation index_ `k` _that corresponds to the highest score we can achieve if we rotated_ `nums` _by it_. If there are multiple answers, return the smallest such index `k`.',\n",
       " 'answer': '```python\\ndef moves_to_chessboard(board):\\n    N = len(board)\\n    row, col = 0, 0\\n    row_count, col_count = 0, 0\\n\\n    for i in range(N):\\n        for j in range(N):\\n            if (board[0][0] ^ board[i][0] ^ board[0][j] ^ board[i][j]) != 0:\\n                return -1\\n\\n    for i in range(N):\\n        row ^= board[0][i]\\n        col ^= board[i][0]\\n        row_count += 1 if board[0][i] == 1 else 0\\n        col_count += 1 if board[i][0] == 1 else 0\\n\\n    if row != 0 and row_count * 2 != N:\\n        return -1\\n    if col != 0 and col_count * 2 != N:\\n        return -1\\n\\n    if N % 2 == 1:\\n        if row_count % 2 == 1:\\n            row_count = N - row_count\\n        if col_count % 2 == 1:\\n            col_count = N - col_count\\n    else:\\n        row_count = min(row_count, N - row_count)\\n        col_count = min(col_count, N - col_count)\\n\\n    return (row_count + col_count) // 2\\n```\\n\\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8c09e-ee35-48ca-a7d3-56de23bf74be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Different Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c291ef2-8c73-41d7-a49a-82d220fd4ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smaller_df(train, test, num_train = 500, seed = 42):\n",
    "    num_test = int(((100*num_train)/80) - num_train) #to get 20% test\n",
    "\n",
    "    train_small = train.shuffle(seed = seed).select(range(num_train))\n",
    "    test_small = test.shuffle(seed = seed).select(range(num_test))\n",
    "    \n",
    "    dict_final =  DatasetDict({\n",
    "    'train': train_small,\n",
    "    'test': test_small\n",
    "})\n",
    "    \n",
    "    return dict_final, train_small, test_small\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "726b2c12-456f-4da1-be2d-a39b68681bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### General Knwoledge/Facts\n",
    "general_dict_small, general_train_small, general_test_small = smaller_df(general_dict['train'],\n",
    "                                                                         general_dict['test'])\n",
    "\n",
    "##Math\n",
    "math_dict_small, math_train_small, math_test_small = smaller_df(math_train, math_test)\n",
    "\n",
    "### Code\n",
    "code_dict_small, code_train_small, code_test_small = smaller_df(code_dict['train'], code_dict['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977fd4e-3fa5-4eff-b855-3af20d8d5288",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c905e62b-8fa0-40a1-8441-041309ba0bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5355b55-7f8e-45a2-b6d1-91357968dedf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_math = math_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code = code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general = general_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_code = general_code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_math = general_math_dict.map(preprocess_function, batched=True)\n",
    "\n",
    "### Smaller Models\n",
    "tokenized_dataset_math_small = math_dict_small.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code_small = code_dict_small.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_small = general_dict_small.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "efdd0266-9885-42ed-8a12-0de5a8cd1070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cd4453a-5b62-4c43-be98-911e0d460396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb6b0-9938-4584-a913-d523357eb3c6",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9a639-1778-4713-abe2-945221bee480",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d461a46b-1029-4a02-9ecf-4bbde555e235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_args(output_dir, L_RATE = 3e-4, BATCH_SIZE = 4, PER_DEVICE_EVAL_BATCH = 4, WEIGHT_DECAY = 0.01, SAVE_TOTAL_LIM = 3, NUM_EPOCHS = 3, OVERWRITE_OUTPUT_DIR = True, LOAD_BEST_MODEL_AT_END = True):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "       output_dir=output_dir,\n",
    "       overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "       save_strategy=\"epoch\",\n",
    "       evaluation_strategy=\"epoch\",\n",
    "       load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "       learning_rate=L_RATE,\n",
    "       per_device_train_batch_size=BATCH_SIZE,\n",
    "       per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "       weight_decay=WEIGHT_DECAY,\n",
    "       save_total_limit=SAVE_TOTAL_LIM,\n",
    "       num_train_epochs=NUM_EPOCHS,\n",
    "       predict_with_generate=True,\n",
    "       push_to_hub=False\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f7a5c09-7624-49f3-992b-1b402c268694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lrs = [3e-3, 3e-4, 3e-5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21401d56-7d78-4038-96a3-f8e0f6b44525",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cb02c39-54c8-4d83-baa1-0ffe30383c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_math)\n",
    "    \n",
    "    output_dir_root = \"./results/math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_math = Seq2SeqTrainer(\n",
    "       model=model_math,\n",
    "       args=training_args_math,\n",
    "       train_dataset=tokenized_dataset_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_math.train()\n",
    "\n",
    "    del model_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841d885-c881-422c-ade2-90c26745c70d",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47a56e89-3e6c-4155-8293-7f7a9bf44d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_code)\n",
    "    \n",
    "    output_dir_root = \"./results/code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_code = Seq2SeqTrainer(\n",
    "       model=model_code,\n",
    "       args=training_args_code,\n",
    "       train_dataset=tokenized_dataset_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_code.train()\n",
    "\n",
    "    del model_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35f10-0a68-4a87-a967-c296629f6b32",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "732bb296-b51f-4715-9b3d-d5800246c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general)\n",
    "\n",
    "    output_dir_root = \"./results/general\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general = Seq2SeqTrainer(\n",
    "       model=model_general,\n",
    "       args=training_args_general,\n",
    "       train_dataset=tokenized_dataset_general[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general.train()\n",
    "\n",
    "    del model_general\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b818c67-6b6b-4ce3-b43a-9a5eb3d57695",
   "metadata": {},
   "source": [
    "### General + Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39dc5804-50f2-46e0-9192-971b0a153f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_general_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_code)\n",
    "\n",
    "    output_dir_root = \"./results/general_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_code = Seq2SeqTrainer(\n",
    "       model=model_general_code,\n",
    "       args=training_args_general_code,\n",
    "       train_dataset=tokenized_dataset_general_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_code.train()\n",
    "\n",
    "    del model_general_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843366a-5d48-4364-bc58-4cccc0f2c5c8",
   "metadata": {},
   "source": [
    "### General + Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c5b87f6-d73f-4aa4-bc80-28ee764a0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_general_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_math)\n",
    "\n",
    "    output_dir_root = \"./results/general_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_math = Seq2SeqTrainer(\n",
    "       model=model_general_math,\n",
    "       args=training_args_general_math,\n",
    "       train_dataset=tokenized_dataset_general_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_math.train()\n",
    "\n",
    "    del model_general_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8eef4a-f4af-48f2-afe0-9a67550e6ce9",
   "metadata": {},
   "source": [
    "### Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d26117-84cf-4db3-9f9e-f7afc999bebc",
   "metadata": {},
   "source": [
    "#### 1. Small Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da559dd3-6d6e-47de-b71e-2c8c30891737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: answer, question. If answer, question are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/emilia/micromamba/envs/cs109b/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n",
      "  Number of trainable parameters = 222903552\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/emilia/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/emilia/Documents/Fall_2023/NLP/NLP_research/wandb/run-20231204_211627-h8p566ia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlops_/huggingface/runs/h8p566ia' target=\"_blank\">worthy-aardvark-1</a></strong> to <a href='https://wandb.ai/mlops_/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlops_/huggingface' target=\"_blank\">https://wandb.ai/mlops_/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlops_/huggingface/runs/h8p566ia' target=\"_blank\">https://wandb.ai/mlops_/huggingface/runs/h8p566ia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14/375 01:07 < 33:46, 0.18 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_small_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_math_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_math)\n",
    "    \n",
    "    output_dir_root = \"./results/small_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_math_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_math_small = Seq2SeqTrainer(\n",
    "       model=model_small_math,\n",
    "       args=training_args_math_small,\n",
    "       train_dataset=tokenized_dataset_math_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_math_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_math_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_math_small.train()\n",
    "\n",
    "    del model_small_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c865f-4ebe-43dd-adb4-c1ba211927a4",
   "metadata": {},
   "source": [
    "### 2. Code Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b80112-b6e6-4715-a5df-37e7b8f011f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_small_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_code_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_code)\n",
    "    \n",
    "    output_dir_root = \"./results/small_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_code_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_code_small = Seq2SeqTrainer(\n",
    "       model=model_small_code,\n",
    "       args=training_args_code_small,\n",
    "       train_dataset=tokenized_dataset_code_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_code_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_code_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_code_small.train()\n",
    "\n",
    "    del model_small_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d566f-797e-4073-8ae4-d56726e10eaf",
   "metadata": {},
   "source": [
    "### 3. General Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8039a-02fd-4d74-8b8f-62e79a021746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    model_small_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_general)\n",
    "\n",
    "    output_dir_root = \"./results/small_general\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_small = Seq2SeqTrainer(\n",
    "       model=model_small_general,\n",
    "       args=training_args_general_small,\n",
    "       train_dataset=tokenized_dataset_general_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_small.train()\n",
    "\n",
    "    del model_small_general\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
