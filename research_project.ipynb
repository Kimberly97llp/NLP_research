{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install tokenizers\n",
    "# !pip install evaluate\n",
    "# !pip install rouge_score\n",
    "# !pip install sentencepiece\n",
    "# !pip install huggingface_hub\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f60ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4539c-b03e-4e00-81da-f9635288d338",
   "metadata": {},
   "source": [
    "Instantiate a T5 tokenizer using the \"t5-base\" pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0a323b-428f-4ca2-88d9-3bd4145031ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f8f8f-24af-4ada-addd-a65f1cda6338",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cb869-dc23-4eff-8438-20e8d767357e",
   "metadata": {},
   "source": [
    "Code to preprocess and create the 5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e5ce-60d4-4910-a253-43809da4be7f",
   "metadata": {},
   "source": [
    "### Create code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb437f7-ae45-41ad-9029-e3b87d133e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_before_example(text):\n",
    "    return text.split(\"**Example 1:**\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5cb0b6-228d-45a0-9e72-4dba309f94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    \"\"\"\n",
    "    Converts data from a JSON file in the specified folder to a Hugging Face Dataset object.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    datasets.Dataset: Hugging Face Dataset object containing 'code_with_problem', 'code_only', and 'question' fields.\n",
    "    \"\"\"\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "    dataset = Dataset.from_pandas(df[['code_with_problem', 'code_only']])\n",
    "    dataset['question'] = dataset['code_with_problem'].apply(extract_text_before_example)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c13de88-ac92-411a-b2e4-f5fbc031f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    \"\"\"\n",
    "    Converts data from a JSON file in the specified folder to a Hugging Face Dataset object.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    datasets.Dataset: Hugging Face Dataset object containing 'question' (extracted from 'code_with_problem') and 'answer' (renamed 'code_only') fields.\n",
    "    \"\"\"\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    # Extract text before \"Example 1\" for the 'code_with_problem' column\n",
    "    df['question'] = df['code_with_problem'].apply(extract_text_before_example)\n",
    "\n",
    "    # Rename columns and create the dataset\n",
    "    df.rename(columns={'code_only': 'answer'}, inplace=True)\n",
    "    dataset = Dataset.from_pandas(df[['question', 'answer']])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7801c5e5-ca37-4247-b821-0e636bde1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = convert_folder_to_datasetdict_code('data/code/leetcode-solutions.json')\n",
    "train_dataset, test_dataset = train_test_split(code_dataset, test_size=0.2)\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03217e55-403e-49f4-809e-98c00df21937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf1fe247-c841-4170-94c8-d97a89877b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = code_dict['train'].num_rows\n",
    "n_test = code_dict['test'].num_rows\n",
    "n_total = n_train + n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a43a410-aa83-46d1-a752-6e3b0387a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are participating in an online chess tournament. There is a chess round that starts every `15` minutes. The first round of the day starts at `00:00`, and after every `15` minutes, a new round starts.\\n\\n*   For example, the second round starts at `00:15`, the fourth round starts at `00:45`, and the seventh round starts at `01:30`.\\n\\nYou are given two strings `loginTime` and `logoutTime` where:\\n\\n*   `loginTime` is the time you will login to the game, and\\n*   `logoutTime` is the time you will logout from the game.\\n\\nIf `logoutTime` is **earlier** than `loginTime`, this means you have played from `loginTime` to midnight and from midnight to `logoutTime`.\\n\\nReturn _the number of full chess rounds you have played in the tournament_.\\n\\n**Note:** All the given times follow the 24-hour clock. That means the first round of the day starts at `00:00` and the last round of the day starts at `23:45`.',\n",
       " 'answer': '```python\\ndef second_largest_digit(s: str) -> int:\\n    largest = -1\\n    second_largest = -1\\n    for c in s:\\n        if c.isdigit():\\n            digit = int(c)\\n            if digit > largest:\\n                second_largest = largest\\n                largest = digit\\n            elif digit != largest and digit > second_largest:\\n                second_largest = digit\\n    return second_largest\\n```\\n\\n'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633e05f-5423-4e20-8c09-1346d19ef328",
   "metadata": {},
   "source": [
    "### Create general knowledge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96b8c412-f8a8-4fa8-ab4a-5bea2ae894a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'data/general/general.csv'\n",
    "general = pd.read_csv(csv_file_path)\n",
    "general = general.sample(n=n_total, random_state=42)\n",
    "general['id'] = range(len(general))\n",
    "train_sample, test_sample = train_test_split(general, test_size=0.2, random_state=42)\n",
    "train_sample.reset_index(drop=True, inplace=True)\n",
    "test_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_sample[['question', 'answer']])\n",
    "test_dataset = Dataset.from_pandas(test_sample[['question', 'answer']])\n",
    "\n",
    "general_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "print(general_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57afb3e1-c17b-4ebe-846b-d82948014f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Add 1 letter to \"Iowa\" to get the name of this tribe who lived south of the Iowa',\n",
       " 'answer': 'Kiowa'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f352-aa2e-430f-b835-239807af143e",
   "metadata": {},
   "source": [
    "### Create math dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64c35803-3c0b-44c0-900b-a5c70864de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_math(folder_path):\n",
    "    #data = {\"id\": [], \"question\": [], \"level\": [], \"type\": [], \"answer\": []}\n",
    "    data = {\"question\": [], \"answer\": []}\n",
    "\n",
    "    subject_dictionary = {\n",
    "        \"algebra\": 1,\n",
    "        \"counting_and_probability\": 2,\n",
    "        \"geometry\": 3,\n",
    "        \"intermediate_algebra\": 4,\n",
    "        \"number_theory\": 5,\n",
    "        \"prealgebra\": 6,\n",
    "        \"precalculus\": 7\n",
    "    }\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                problem = json_data.get(\"problem\", \"\")\n",
    "                level = json_data.get(\"level\", \"\")\n",
    "                type_ = json_data.get(\"type\", \"\")\n",
    "                solution = json_data.get(\"solution\", \"\")\n",
    "\n",
    "                # Generate id from subject code and file name\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                subject_code = subject_dictionary.get(folder_name, 0)  # Default to 0 if not found\n",
    "                id_ = f\"{subject_code}_{file_name}\"\n",
    "\n",
    "                #data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(problem)\n",
    "                #data[\"level\"].append(level)\n",
    "                #data[\"type\"].append(type_)\n",
    "                data[\"answer\"].append(solution)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee114fa9-509b-47d1-9039-ca0679324080",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train = convert_folder_to_datasetdict_math(\"data/math/train/\")\n",
    "math_test = convert_folder_to_datasetdict_math(\"data/math/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820af25a-2806-4cbe-be23-107fc02f632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "math_train = math_train.shuffle(seed=42)\n",
    "math_train = math_train.select(range(n_train))\n",
    "\n",
    "math_test = math_test.shuffle(seed=42)\n",
    "math_test = math_test.select(range(n_test))\n",
    "\n",
    "math_dict = DatasetDict({\n",
    "    'train': math_train,\n",
    "    'test': math_test\n",
    "})\n",
    "\n",
    "print(math_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a46830-2716-495d-a669-87c046816932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the number of units in the distance between $(2,5)$ and $(-6,-1)$?',\n",
       " 'answer': 'We use the distance formula: $\\\\sqrt{(-6 - 2)^2 + (-1 - 5)^2},$ so then we find that $\\\\sqrt{64 + 36} = \\\\boxed{10}$.\\n\\n- OR -\\n\\nWe note that the points $(2, 5)$, $(-6, -1)$, and $(2, -1)$ form a right triangle with legs of length 6 and 8. This is a Pythagorean triple, so the length of the hypotenuse must be $\\\\boxed{10}$.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79018d-53d9-4042-98d7-3f715aea4ca9",
   "metadata": {},
   "source": [
    "### Create 50% samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a08f41-e938-4664-a5d6-15f6a8e8d758",
   "metadata": {},
   "source": [
    "Preprocessing to create 50% samples. Will be used to created the 50% general and 50% math, and the 50% general and 50% code datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22e4c8b9-adfa-4f8a-a7ad-c0a7a3504f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train_sample = math_dict['train'].shuffle(seed=42).select([i for i in range(math_dict['train'].num_rows // 2)])\n",
    "math_test_sample = math_dict['test'].shuffle(seed=42).select([i for i in range(math_dict['test'].num_rows // 2)])\n",
    "\n",
    "general_train_sample = general_dict['train'].shuffle(seed=42).select([i for i in range(general_dict['train'].num_rows // 2+1)])\n",
    "general_test_sample = general_dict['test'].shuffle(seed=42).select([i for i in range(general_dict['test'].num_rows // 2)])\n",
    "\n",
    "code_train_sample = code_dict['train'].shuffle(seed=42).select([i for i in range(code_dict['train'].num_rows // 2)])\n",
    "code_test_sample = code_dict['test'].shuffle(seed=42).select([i for i in range(code_dict['test'].num_rows // 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74577ff-b47a-4c4a-bb30-779c72336122",
   "metadata": {},
   "source": [
    "### Create 50% general 50% math dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5cbcd89-e9ff-4f64-b38f-8b82f96a31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_math_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': math_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': math_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': math_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': math_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "\n",
    "general_math_dict = DatasetDict({\n",
    "    'train': general_math_dict['train'].shuffle(seed=42),\n",
    "    'test': general_math_dict['test'].shuffle(seed=42)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "882a1fc2-3d94-4836-8a1d-c2eea04fd356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the monic quadratic polynomial, in $x,$ with real coefficients, which has $1 - i$ as a root.',\n",
       " 'answer': 'If a polynomial has real coefficients, then any complex conjugate of a root must also be a root.  Hence, the other root is $1 + i.$  Thus, the polynomial is\\n\\\\[(x - 1 - i)(x - 1 + i) = (x - 1)^2 - i^2 = \\\\boxed{x^2 - 2x + 2}.\\\\]'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b6262-6ee4-43e1-bd63-564fc5817642",
   "metadata": {},
   "source": [
    "### Create 50% general 50% code dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a6fb1f1-c1f0-4860-9f28-e0229cde7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "general_code_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': code_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': code_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': code_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': code_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "general_code_dict = DatasetDict({\n",
    "    'train': general_code_dict['train'].shuffle(seed=42),\n",
    "    'test': general_code_dict['test'].shuffle(seed=42)\n",
    "})\n",
    "\n",
    "print(general_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc493dae-6232-483a-98b7-2413a0148097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are given a `m x n` matrix `grid` consisting of **non-negative** integers where `grid[row][col]` represents the **minimum** time required to be able to visit the cell `(row, col)`, which means you can visit the cell `(row, col)` only when the time you visit it is greater than or equal to `grid[row][col]`.\\n\\nYou are standing in the **top-left** cell of the matrix in the `0th` second, and you must move to **any** adjacent cell in the four directions: up, down, left, and right. Each move you make takes 1 second.\\n\\nReturn _the **minimum** time required in which you can visit the bottom-right cell of the matrix_. If you cannot visit the bottom-right cell, then return `-1`.',\n",
       " 'answer': '```python\\nfrom collections import deque\\n\\ndef minTime(grid: list[list[int]]) -> int:\\n    m, n = len(grid), len(grid[0])\\n    visited = [[1000000] * n for _ in range(m)]\\n\\n    dx = [-1, 0, 1, 0]\\n    dy = [0, 1, 0, -1]\\n\\n    visited[0][0] = 0\\n\\n    q = deque([(0, 0)])\\n\\n    while q:\\n        x, y = q.popleft()\\n\\n        for i in range(4):\\n            nx, ny = x + dx[i], y + dy[i]\\n\\n            if 0 <= nx < m and 0 <= ny < n:\\n                t = max(grid[nx][ny], visited[x][y] + 1)\\n                if visited[nx][ny] > t:\\n                    visited[nx][ny] = t\\n                    q.append((nx, ny))\\n\\n    return visited[m - 1][n - 1] if visited[m - 1][n - 1] != 1000000 else -1\\n```\\n\\n'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e5868",
   "metadata": {},
   "source": [
    "## Different Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b67f51-f4d1-4f37-87e9-33afcce91005",
   "metadata": {},
   "source": [
    "Preprocessing to create smaller versions of the datasets (fewer number of fine-tuning examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "262a363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smaller_df(train, test, num_train = 500, seed = 42):\n",
    "    \"\"\"\n",
    "    Creates smaller training and testing datasets by randomly selecting a specified number of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - train (datasets.Dataset): Original training dataset.\n",
    "    - test (datasets.Dataset): Original testing dataset.\n",
    "    - num_train (int): Number of samples to include in the smaller training dataset.\n",
    "    - seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[datasets.DatasetDict, datasets.Dataset, datasets.Dataset]: A DatasetDict containing 'train' and 'test' keys, and the smaller training and testing datasets.\n",
    "    \"\"\"\n",
    "    num_test = int(((100*num_train)/80) - num_train) #to get 20% test\n",
    "\n",
    "    train_small = train.shuffle(seed = seed).select(range(num_train))\n",
    "    test_small = test.shuffle(seed = seed).select(range(num_test))\n",
    "    \n",
    "    dict_final =  DatasetDict({\n",
    "    'train': train_small,\n",
    "    'test': test_small\n",
    "})\n",
    "    \n",
    "    return dict_final, train_small, test_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99a9228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Knowledge/Facts\n",
    "general_dict_small, general_train_small, general_test_small = smaller_df(general_dict['train'],\n",
    "                                                                         general_dict['test'])\n",
    "# Math\n",
    "math_dict_small, math_train_small, math_test_small = smaller_df(math_train, math_test)\n",
    "\n",
    "#  Code\n",
    "code_dict_small, code_train_small, code_test_small = smaller_df(code_dict['train'], code_dict['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977fd4e-3fa5-4eff-b855-3af20d8d5288",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab26e3",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35138741",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Add prefix to the sentences, tokenize the text, and set the labels.\n",
    "\n",
    "    Parameters:\n",
    "    - examples (dict): A dictionary containing 'question' and 'answer' fields.\n",
    "\n",
    "    Returns:\n",
    "    dict: Tokenized model inputs with added prefix and labels.\n",
    "    \"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc36e7b",
   "metadata": {},
   "source": [
    "### Extension - Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c905e62b-8fa0-40a1-8441-041309ba0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question while paying attention to how variables are used to track information: \"\n",
    "\n",
    "def preprocess_function2(examples):\n",
    "    \"\"\"\n",
    "    Add prefix to the sentences, tokenize the text, and set the labels.\n",
    "\n",
    "    Parameters:\n",
    "    - examples (dict): A dictionary containing 'question' and 'answer' fields.\n",
    "\n",
    "    Returns:\n",
    "    dict: Tokenized model inputs with added prefix and labels.\n",
    "    \"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5355b55-7f8e-45a2-b6d1-91357968dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]Map: 100%|██████████| 1887/1887 [00:00<00:00, 2173.53 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 2383.41 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 2005.92 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 1820.35 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 8841.24 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 8733.93 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 2998.78 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 3199.70 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 3656.19 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 3766.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_math = math_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code = code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general = general_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_code = general_code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_math = general_math_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4033af",
   "metadata": {},
   "source": [
    "### Smaller Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a4db4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2127.33 examples/s]\n",
      "Map: 100%|██████████| 125/125 [00:00<00:00, 2292.92 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2109.43 examples/s]\n",
      "Map: 100%|██████████| 125/125 [00:00<00:00, 1999.54 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9171.13 examples/s]\n",
      "Map: 100%|██████████| 125/125 [00:00<00:00, 7812.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_math_small = math_dict_small.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code_small = code_dict_small.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_small = general_dict_small.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efdd0266-9885-42ed-8a12-0de5a8cd1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cd4453a-5b62-4c43-be98-911e0d460396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - eval_preds (tuple): Tuple containing predictions (preds) and labels.\n",
    "\n",
    "    Returns:\n",
    "    dict: Computed metrics using RougeLSum with newline-separated sentences.\n",
    "    \"\"\"\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb6b0-9938-4584-a913-d523357eb3c6",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9a639-1778-4713-abe2-945221bee480",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d461a46b-1029-4a02-9ecf-4bbde555e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_args(output_dir, L_RATE = 3e-4, BATCH_SIZE = 4, PER_DEVICE_EVAL_BATCH = 4, WEIGHT_DECAY = 0.01, SAVE_TOTAL_LIM = 3, NUM_EPOCHS = 3, OVERWRITE_OUTPUT_DIR = True, LOAD_BEST_MODEL_AT_END = True):\n",
    "    \"\"\"\n",
    "    Define training arguments for Seq2Seq training.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir (str): Output directory for saving model checkpoints and logs.\n",
    "    - L_RATE (float): Learning rate for training.\n",
    "    - BATCH_SIZE (int): Batch size for training.\n",
    "    - PER_DEVICE_EVAL_BATCH (int): Batch size for evaluation.\n",
    "    - WEIGHT_DECAY (float): Weight decay for optimization.\n",
    "    - SAVE_TOTAL_LIM (int): Total number of checkpoints to save.\n",
    "    - NUM_EPOCHS (int): Number of training epochs.\n",
    "    - OVERWRITE_OUTPUT_DIR (bool): Whether to overwrite the output directory if it exists.\n",
    "    - LOAD_BEST_MODEL_AT_END (bool): Whether to load the best model at the end of training.\n",
    "\n",
    "    Returns:\n",
    "    transformers.Seq2SeqTrainingArguments: Training arguments for Seq2Seq model.\n",
    "    \"\"\"\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "       output_dir=output_dir,\n",
    "       overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "       save_strategy=\"epoch\",\n",
    "       evaluation_strategy=\"epoch\",\n",
    "       load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "       learning_rate=L_RATE,\n",
    "       per_device_train_batch_size=BATCH_SIZE,\n",
    "       per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "       weight_decay=WEIGHT_DECAY,\n",
    "       save_total_limit=SAVE_TOTAL_LIM,\n",
    "       num_train_epochs=NUM_EPOCHS,\n",
    "       predict_with_generate=True,\n",
    "       push_to_hub=False\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f7a5c09-7624-49f3-992b-1b402c268694",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3e-3, 3e-4, 3e-5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21401d56-7d78-4038-96a3-f8e0f6b44525",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cb02c39-54c8-4d83-baa1-0ffe30383c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231213_231725-a0o7fim8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/a0o7fim8' target=\"_blank\">floral-firebrand-22</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/a0o7fim8' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/a0o7fim8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:26<02:55,  5.37it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:08<02:55,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.157402276992798, 'eval_rouge1': 0.15791446753160732, 'eval_rouge2': 0.06253428545548179, 'eval_rougeL': 0.13231324716097564, 'eval_rougeLsum': 0.1450682450058146, 'eval_runtime': 41.9676, 'eval_samples_per_second': 11.247, 'eval_steps_per_second': 2.812, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:25<03:29,  4.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8422, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:46<01:07,  6.95it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:26<01:07,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.022892475128174, 'eval_rouge1': 0.143764168423235, 'eval_rouge2': 0.05453990187547328, 'eval_rougeL': 0.12376864720037017, 'eval_rougeLsum': 0.13246826930254563, 'eval_runtime': 39.6309, 'eval_samples_per_second': 11.91, 'eval_steps_per_second': 2.977, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [05:01<01:10,  5.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3288, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [06:16<00:00,  5.14it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [06:59<00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9891810417175293, 'eval_rouge1': 0.14696774065600438, 'eval_rouge2': 0.056186690687112587, 'eval_rougeL': 0.12576572183593526, 'eval_rougeLsum': 0.13543265872320354, 'eval_runtime': 42.8265, 'eval_samples_per_second': 11.021, 'eval_steps_per_second': 2.755, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [07:15<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 438.19, 'train_samples_per_second': 12.919, 'train_steps_per_second': 3.231, 'train_loss': 2.4877741754391773, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_math)\n",
    "    \n",
    "    output_dir_root = \"./results/prompt_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_math = Seq2SeqTrainer(\n",
    "       model=model_math,\n",
    "       args=training_args_math,\n",
    "       train_dataset=tokenized_dataset_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_math.train()\n",
    "\n",
    "    del model_math, trainer_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841d885-c881-422c-ade2-90c26745c70d",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47a56e89-3e6c-4155-8293-7f7a9bf44d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:08<02:22,  6.64it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:47<02:22,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1423404216766357, 'eval_rouge1': 0.1182444351868111, 'eval_rouge2': 0.039196967880248804, 'eval_rougeL': 0.11625639712495212, 'eval_rougeLsum': 0.11622626648431872, 'eval_runtime': 38.4477, 'eval_samples_per_second': 12.276, 'eval_steps_per_second': 3.069, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:15<02:17,  6.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9453, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:20<01:09,  6.80it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [04:02<01:09,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9248026609420776, 'eval_rouge1': 0.11479931025698409, 'eval_rouge2': 0.04229243095290905, 'eval_rougeL': 0.1132031791854636, 'eval_rougeLsum': 0.1130226621738151, 'eval_runtime': 41.8051, 'eval_samples_per_second': 11.29, 'eval_steps_per_second': 2.823, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:29<00:56,  7.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2719, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:29<00:00,  7.46it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [06:08<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8727754354476929, 'eval_rouge1': 0.10983940613667617, 'eval_rouge2': 0.04154851600131902, 'eval_rougeL': 0.10833415097896029, 'eval_rougeLsum': 0.10812533490939708, 'eval_runtime': 38.956, 'eval_samples_per_second': 12.116, 'eval_steps_per_second': 3.029, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:32<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 392.7687, 'train_samples_per_second': 14.413, 'train_steps_per_second': 3.605, 'train_loss': 2.4671413184559277, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_code)\n",
    "    \n",
    "    output_dir_root = \"./results/prompt_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_code = Seq2SeqTrainer(\n",
    "       model=model_code,\n",
    "       args=training_args_code,\n",
    "       train_dataset=tokenized_dataset_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_code.train()\n",
    "\n",
    "    del model_code, trainer_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35f10-0a68-4a87-a967-c296629f6b32",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bb296-b51f-4715-9b3d-d5800246c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general)\n",
    "\n",
    "    output_dir_root = \"./results/general\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general = Seq2SeqTrainer(\n",
    "       model=model_general,\n",
    "       args=training_args_general,\n",
    "       train_dataset=tokenized_dataset_general[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general.train()\n",
    "\n",
    "    del model_general, trainer_general\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b818c67-6b6b-4ce3-b43a-9a5eb3d57695",
   "metadata": {},
   "source": [
    "### General + Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39dc5804-50f2-46e0-9192-971b0a153f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231204_220058-92x30oln</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln' target=\"_blank\">lilac-planet-18</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:05<02:13,  7.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [01:43<02:13,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.132077693939209, 'eval_rouge1': 0.055051808944643704, 'eval_rouge2': 0.016222703488948487, 'eval_rougeL': 0.05433968281931898, 'eval_rougeLsum': 0.05441902197562565, 'eval_runtime': 38.6494, 'eval_samples_per_second': 12.212, 'eval_steps_per_second': 3.053, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:10<02:00,  7.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6281, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:10<01:12,  6.51it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:46<01:12,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8248320817947388, 'eval_rouge1': 0.07404124374521334, 'eval_rouge2': 0.019232725796420602, 'eval_rougeL': 0.07357504130459036, 'eval_rougeLsum': 0.07368692572109758, 'eval_runtime': 36.6937, 'eval_samples_per_second': 12.863, 'eval_steps_per_second': 3.216, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:05<00:53,  7.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7786, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:00<00:00,  7.48it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [05:37<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.736644983291626, 'eval_rouge1': 0.07237803222073752, 'eval_rouge2': 0.020723353671982178, 'eval_rougeL': 0.07175227425435451, 'eval_rougeLsum': 0.07182516121018465, 'eval_runtime': 36.3015, 'eval_samples_per_second': 13.002, 'eval_steps_per_second': 3.251, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:48<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 351.6636, 'train_samples_per_second': 16.098, 'train_steps_per_second': 4.027, 'train_loss': 1.9456297017760196, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:03<01:56,  8.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [01:38<01:56,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.760346531867981, 'eval_rouge1': 0.12132619838044889, 'eval_rouge2': 0.0371879520789538, 'eval_rougeL': 0.1208510402218295, 'eval_rougeLsum': 0.1210844252051826, 'eval_runtime': 34.9811, 'eval_samples_per_second': 13.493, 'eval_steps_per_second': 3.373, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [01:49<01:55,  7.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3452, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [02:47<01:11,  6.58it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:23<01:11,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5788134336471558, 'eval_rouge1': 0.11538274777158411, 'eval_rouge2': 0.03491347388730446, 'eval_rougeL': 0.11516147987108587, 'eval_rougeLsum': 0.11438624150784589, 'eval_runtime': 36.8886, 'eval_samples_per_second': 12.795, 'eval_steps_per_second': 3.199, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [03:42<00:53,  7.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.659, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [04:37<00:00,  7.26it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [05:14<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5343466997146606, 'eval_rouge1': 0.12369657569645576, 'eval_rouge2': 0.038815463721845155, 'eval_rougeL': 0.12219236045938289, 'eval_rougeLsum': 0.12186390364948171, 'eval_runtime': 36.8111, 'eval_samples_per_second': 12.822, 'eval_steps_per_second': 3.206, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 322.2697, 'train_samples_per_second': 17.566, 'train_steps_per_second': 4.394, 'train_loss': 1.8327802668857036, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:06<02:24,  6.52it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:41<02:24,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3541741371154785, 'eval_rouge1': 0.10543474610190923, 'eval_rouge2': 0.027912390306222754, 'eval_rougeL': 0.104541700149297, 'eval_rougeLsum': 0.10379574014373033, 'eval_runtime': 35.2405, 'eval_samples_per_second': 13.394, 'eval_steps_per_second': 3.348, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [01:59<02:44,  5.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1622, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:04<01:12,  6.55it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [03:40<01:12,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1470742225646973, 'eval_rouge1': 0.11440688184860422, 'eval_rouge2': 0.03585640348614795, 'eval_rougeL': 0.11380251997686144, 'eval_rougeLsum': 0.11309426185751145, 'eval_runtime': 36.0755, 'eval_samples_per_second': 13.084, 'eval_steps_per_second': 3.271, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:03<01:09,  5.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4896, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:18<00:00,  6.22it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:54<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0940744876861572, 'eval_rouge1': 0.11471425599899554, 'eval_rouge2': 0.03639041958518499, 'eval_rougeL': 0.11419256284976584, 'eval_rougeLsum': 0.11350022159156378, 'eval_runtime': 36.0568, 'eval_samples_per_second': 13.09, 'eval_steps_per_second': 3.273, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:11<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 370.9641, 'train_samples_per_second': 15.26, 'train_steps_per_second': 3.817, 'train_loss': 2.68684063927602, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [06:11<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_code)\n",
    "\n",
    "    output_dir_root = \"./results/general_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_code = Seq2SeqTrainer(\n",
    "       model=model_general_code,\n",
    "       args=training_args_general_code,\n",
    "       train_dataset=tokenized_dataset_general_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_code.train()\n",
    "\n",
    "    del model_general_code, trainer_general_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c56066",
   "metadata": {},
   "source": [
    "### General + Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c5b87f6-d73f-4aa4-bc80-28ee764a0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231204_222254-yv29x7g0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0' target=\"_blank\">good-jazz-19</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:30<02:35,  6.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:07<02:35,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5402798652648926, 'eval_rouge1': 0.032277776555268506, 'eval_rouge2': 0.0029141143878364728, 'eval_rougeL': 0.03204248391573171, 'eval_rougeLsum': 0.031950849918273065, 'eval_runtime': 37.0247, 'eval_samples_per_second': 12.748, 'eval_steps_per_second': 3.187, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:32<02:17,  6.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0606, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:39<01:17,  6.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:16<01:17,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3229095935821533, 'eval_rouge1': 0.032444724403872735, 'eval_rouge2': 0.0035144391149949, 'eval_rougeL': 0.03171422707637668, 'eval_rougeLsum': 0.03172260187056673, 'eval_runtime': 36.9298, 'eval_samples_per_second': 12.781, 'eval_steps_per_second': 3.195, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:32<01:09,  6.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2633, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:35<00:00,  6.53it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [06:11<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.247628927230835, 'eval_rouge1': 0.039121258214399816, 'eval_rouge2': 0.0032839848504259083, 'eval_rougeL': 0.03872789528552882, 'eval_rougeLsum': 0.038698280799125157, 'eval_runtime': 35.8744, 'eval_samples_per_second': 13.157, 'eval_steps_per_second': 3.289, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 385.1121, 'train_samples_per_second': 14.7, 'train_steps_per_second': 3.677, 'train_loss': 2.4215501257255254, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:12<02:38,  5.97it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:48<02:38,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9895418882369995, 'eval_rouge1': 0.1320215180700664, 'eval_rouge2': 0.03874952300747349, 'eval_rougeL': 0.12353501415658114, 'eval_rougeLsum': 0.1275214497003628, 'eval_runtime': 36.0791, 'eval_samples_per_second': 13.082, 'eval_steps_per_second': 3.271, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:00<02:13,  6.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4809, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:07<01:12,  6.51it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [03:44<01:12,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8589943647384644, 'eval_rouge1': 0.135461050614511, 'eval_rouge2': 0.038585915833919224, 'eval_rougeL': 0.12459804955343722, 'eval_rougeLsum': 0.12869890674568712, 'eval_runtime': 36.548, 'eval_samples_per_second': 12.915, 'eval_steps_per_second': 3.229, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [03:55<01:08,  6.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9168, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [04:58<00:00,  6.85it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:35<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8303388357162476, 'eval_rouge1': 0.14259260962717454, 'eval_rouge2': 0.04300571794768096, 'eval_rougeL': 0.13308619842145114, 'eval_rougeLsum': 0.1364182526233655, 'eval_runtime': 36.3798, 'eval_samples_per_second': 12.974, 'eval_steps_per_second': 3.244, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:42<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 342.522, 'train_samples_per_second': 16.527, 'train_steps_per_second': 4.134, 'train_loss': 2.0566785995569608, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:12<02:35,  6.06it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:48<02:35,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3861546516418457, 'eval_rouge1': 0.1119125107451445, 'eval_rouge2': 0.0407551038633739, 'eval_rougeL': 0.10108785562620667, 'eval_rougeLsum': 0.10601038009687778, 'eval_runtime': 36.2431, 'eval_samples_per_second': 13.023, 'eval_steps_per_second': 3.256, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:01<02:24,  6.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0762, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:09<01:13,  6.42it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:46<01:13,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2488908767700195, 'eval_rouge1': 0.12060288745848656, 'eval_rouge2': 0.039178581996189996, 'eval_rougeL': 0.11036107868460496, 'eval_rougeLsum': 0.11576035112598533, 'eval_runtime': 37.3415, 'eval_samples_per_second': 12.64, 'eval_steps_per_second': 3.16, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:03<01:13,  5.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5417, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:09<00:00,  6.60it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:46<00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2149879932403564, 'eval_rouge1': 0.11827782446039363, 'eval_rouge2': 0.0367991779285983, 'eval_rougeL': 0.1077602373969512, 'eval_rougeLsum': 0.11287944637073197, 'eval_runtime': 37.475, 'eval_samples_per_second': 12.595, 'eval_steps_per_second': 3.149, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:49<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 349.4659, 'train_samples_per_second': 16.199, 'train_steps_per_second': 4.052, 'train_loss': 2.6996212544414284, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_math)\n",
    "\n",
    "    output_dir_root = \"./results/general_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_math = Seq2SeqTrainer(\n",
    "       model=model_general_math,\n",
    "       args=training_args_general_math,\n",
    "       train_dataset=tokenized_dataset_general_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_math.train()\n",
    "\n",
    "    del model_general_math, trainer_general_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27e14e",
   "metadata": {},
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0b2afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3e-4, 3e-5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbfad5",
   "metadata": {},
   "source": [
    "### Small Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aee327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231213_223501-u7u0bett</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/u7u0bett' target=\"_blank\">swept-cosmos-21</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/u7u0bett' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/u7u0bett</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 125/375 [00:23<00:48,  5.17it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 33%|███▎      | 125/375 [00:34<00:48,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0678186416625977, 'eval_rouge1': 0.14753079887204634, 'eval_rouge2': 0.05311448081010642, 'eval_rougeL': 0.12152703358065112, 'eval_rougeLsum': 0.13587401576603353, 'eval_runtime': 11.2295, 'eval_samples_per_second': 11.131, 'eval_steps_per_second': 2.85, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [01:12<00:26,  4.68it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 67%|██████▋   | 250/375 [01:24<00:26,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9126132726669312, 'eval_rouge1': 0.15403537605975903, 'eval_rouge2': 0.06527070297054133, 'eval_rougeL': 0.13088972856643843, 'eval_rougeLsum': 0.1433835411893397, 'eval_runtime': 11.7696, 'eval_samples_per_second': 10.621, 'eval_steps_per_second': 2.719, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:58<00:00,  5.60it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "100%|██████████| 375/375 [02:09<00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.875769019126892, 'eval_rouge1': 0.14871922689408157, 'eval_rouge2': 0.061755863660551844, 'eval_rougeL': 0.1268634629697864, 'eval_rougeLsum': 0.13713029994356685, 'eval_runtime': 11.2254, 'eval_samples_per_second': 11.135, 'eval_steps_per_second': 2.851, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [02:11<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 134.7008, 'train_samples_per_second': 11.136, 'train_steps_per_second': 2.784, 'train_loss': 2.2278942057291666, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 125/375 [00:23<00:50,  4.96it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 125/375 [00:34<00:50,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5613465309143066, 'eval_rouge1': 0.124674201440485, 'eval_rouge2': 0.048904178710419695, 'eval_rougeL': 0.10723991623451987, 'eval_rougeLsum': 0.11489920487529967, 'eval_runtime': 11.6595, 'eval_samples_per_second': 10.721, 'eval_steps_per_second': 2.745, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [01:09<00:22,  5.55it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 250/375 [01:20<00:22,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4130988121032715, 'eval_rouge1': 0.1486222523556518, 'eval_rouge2': 0.0655636808190668, 'eval_rougeL': 0.12396740526687447, 'eval_rougeLsum': 0.13558528422910707, 'eval_runtime': 11.1454, 'eval_samples_per_second': 11.215, 'eval_steps_per_second': 2.871, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:46<00:00,  5.31it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 375/375 [01:58<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.375945806503296, 'eval_rouge1': 0.1529917901464747, 'eval_rouge2': 0.06731742202098422, 'eval_rougeL': 0.1275433978983742, 'eval_rougeLsum': 0.140074213341659, 'eval_runtime': 12.2609, 'eval_samples_per_second': 10.195, 'eval_steps_per_second': 2.61, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [02:19<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 139.3229, 'train_samples_per_second': 10.766, 'train_steps_per_second': 2.692, 'train_loss': 3.0271315104166665, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_small_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_math_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_math)\n",
    "    \n",
    "    output_dir_root = \"./results/small_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_math_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_math_small = Seq2SeqTrainer(\n",
    "       model=model_small_math,\n",
    "       args=training_args_math_small,\n",
    "       train_dataset=tokenized_dataset_math_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_math_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_math_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_math_small.train()\n",
    "\n",
    "    del model_small_math, trainer_math_small\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89884a",
   "metadata": {},
   "source": [
    "### Small Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06caf651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 125/375 [00:18<00:34,  7.34it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 125/375 [00:29<00:34,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.016775369644165, 'eval_rouge1': 0.11612913432142086, 'eval_rouge2': 0.04115832449191913, 'eval_rougeL': 0.11456502677605027, 'eval_rougeLsum': 0.11431124814196948, 'eval_runtime': 10.6537, 'eval_samples_per_second': 11.733, 'eval_steps_per_second': 3.004, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [01:09<00:18,  6.64it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 250/375 [01:20<00:18,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7889060974121094, 'eval_rouge1': 0.11480620883423139, 'eval_rouge2': 0.04466836894982739, 'eval_rougeL': 0.11265325718667234, 'eval_rougeLsum': 0.11256915855636684, 'eval_runtime': 10.3492, 'eval_samples_per_second': 12.078, 'eval_steps_per_second': 3.092, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:45<00:00,  6.40it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "100%|██████████| 375/375 [01:56<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7229335308074951, 'eval_rouge1': 0.11195872617542241, 'eval_rouge2': 0.04489759086861367, 'eval_rougeL': 0.11069500056759757, 'eval_rougeLsum': 0.11065847908914592, 'eval_runtime': 11.2752, 'eval_samples_per_second': 11.086, 'eval_steps_per_second': 2.838, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [02:12<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 132.309, 'train_samples_per_second': 11.337, 'train_steps_per_second': 2.834, 'train_loss': 2.09246728515625, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 125/375 [00:33<00:50,  4.97it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 125/375 [00:46<00:50,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.841360569000244, 'eval_rouge1': 0.10304150997549864, 'eval_rouge2': 0.03568081898237045, 'eval_rougeL': 0.10188450896375748, 'eval_rougeLsum': 0.10186907991614094, 'eval_runtime': 13.5703, 'eval_samples_per_second': 9.211, 'eval_steps_per_second': 2.358, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [01:26<00:26,  4.65it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 250/375 [01:39<00:26,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.602370262145996, 'eval_rouge1': 0.12498260261167471, 'eval_rouge2': 0.03906354867748432, 'eval_rougeL': 0.12195713295643335, 'eval_rougeLsum': 0.12199950239308704, 'eval_runtime': 12.1936, 'eval_samples_per_second': 10.251, 'eval_steps_per_second': 2.624, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [02:19<00:00,  4.86it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 375/375 [02:31<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5382773876190186, 'eval_rouge1': 0.12598068779024985, 'eval_rouge2': 0.040002422760855236, 'eval_rougeL': 0.12354459804026416, 'eval_rougeLsum': 0.12334207922655581, 'eval_runtime': 11.4736, 'eval_samples_per_second': 10.895, 'eval_steps_per_second': 2.789, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [02:54<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 174.6364, 'train_samples_per_second': 8.589, 'train_steps_per_second': 2.147, 'train_loss': 3.161347330729167, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [02:55<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_small_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_code_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_code)\n",
    "    \n",
    "    output_dir_root = \"./results/small_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_code_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_code_small = Seq2SeqTrainer(\n",
    "       model=model_small_code,\n",
    "       args=training_args_code_small,\n",
    "       train_dataset=tokenized_dataset_code_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_code_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_code_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_code_small.train()\n",
    "\n",
    "    del model_small_code, trainer_code_small\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08578daf",
   "metadata": {},
   "source": [
    "### Small General Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4ff8b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 125/375 [00:14<00:30,  8.27it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 33%|███▎      | 125/375 [00:19<00:30,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.662008762359619, 'eval_rouge1': 0.11345518925518927, 'eval_rouge2': 0.026666666666666665, 'eval_rougeL': 0.11248473748473747, 'eval_rougeLsum': 0.11274676434676437, 'eval_runtime': 4.4644, 'eval_samples_per_second': 27.999, 'eval_steps_per_second': 7.168, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [00:52<00:13,  9.15it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 67%|██████▋   | 250/375 [00:56<00:13,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.707836389541626, 'eval_rouge1': 0.13003809523809523, 'eval_rouge2': 0.029333333333333333, 'eval_rougeL': 0.12880317460317464, 'eval_rougeLsum': 0.13023809523809526, 'eval_runtime': 3.8162, 'eval_samples_per_second': 32.755, 'eval_steps_per_second': 8.385, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:23<00:00,  8.46it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "100%|██████████| 375/375 [01:27<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7521719932556152, 'eval_rouge1': 0.13297633477633478, 'eval_rouge2': 0.03911111111111112, 'eval_rougeL': 0.133212987012987, 'eval_rougeLsum': 0.13399278499278505, 'eval_runtime': 4.3759, 'eval_samples_per_second': 28.566, 'eval_steps_per_second': 7.313, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [01:36<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 96.4893, 'train_samples_per_second': 15.546, 'train_steps_per_second': 3.886, 'train_loss': 2.331829264322917, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:36<00:00,  3.87it/s]\n",
      " 33%|███▎      | 125/375 [00:15<00:30,  8.25it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 125/375 [00:20<00:30,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.800891637802124, 'eval_rouge1': 0.06090000000000001, 'eval_rouge2': 0.015238095238095238, 'eval_rougeL': 0.06083333333333333, 'eval_rougeLsum': 0.061333333333333344, 'eval_runtime': 4.6068, 'eval_samples_per_second': 27.134, 'eval_steps_per_second': 6.946, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [00:42<00:14,  8.40it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 250/375 [00:46<00:14,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.699368476867676, 'eval_rouge1': 0.08234285714285713, 'eval_rouge2': 0.019999999999999997, 'eval_rougeL': 0.08327619047619048, 'eval_rougeLsum': 0.08407619047619047, 'eval_runtime': 4.142, 'eval_samples_per_second': 30.179, 'eval_steps_per_second': 7.726, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [01:11<00:00,  8.44it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 375/375 [01:16<00:00,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.682535171508789, 'eval_rouge1': 0.10452380952380952, 'eval_rouge2': 0.03047619047619048, 'eval_rougeL': 0.10563809523809525, 'eval_rougeLsum': 0.10454285714285715, 'eval_runtime': 4.1929, 'eval_samples_per_second': 29.812, 'eval_steps_per_second': 7.632, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 375/375 [01:40<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 100.6401, 'train_samples_per_second': 14.905, 'train_steps_per_second': 3.726, 'train_loss': 3.2527493489583335, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_small_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_small = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_small_general)\n",
    "\n",
    "    output_dir_root = \"./results/small_general\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_small = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_small = Seq2SeqTrainer(\n",
    "       model=model_small_general,\n",
    "       args=training_args_general_small,\n",
    "       train_dataset=tokenized_dataset_general_small[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_small[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_small,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_small.train()\n",
    "\n",
    "    del model_small_general, trainer_general_small\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
