{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd6376-ecd5-417c-a171-7dab4d83bcf1",
   "metadata": {},
   "source": [
    "# 6.8610  PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8fd1-ac41-4d6b-a093-d2521f337837",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69c3a0d-afdb-483d-9b58-506857a470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install tokenizers\n",
    "# !pip install evaluate\n",
    "# !pip install rouge_score\n",
    "# !pip install sentencepiece\n",
    "# !pip install huggingface_hub\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673e826c-17f9-45cf-b600-a0a41d8a33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f60ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64e28-68b5-489d-97b4-139cbb60c837",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0a323b-428f-4ca2-88d9-3bd4145031ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f8f8f-24af-4ada-addd-a65f1cda6338",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e5ce-60d4-4910-a253-43809da4be7f",
   "metadata": {},
   "source": [
    "### Create code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb437f7-ae45-41ad-9029-e3b87d133e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_before_example(text):\n",
    "    return text.split(\"**Example 1:**\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5cb0b6-228d-45a0-9e72-4dba309f94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "    dataset = Dataset.from_pandas(df[['code_with_problem', 'code_only']])\n",
    "    dataset['question'] = dataset['code_with_problem'].apply(extract_text_before_example)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c13de88-ac92-411a-b2e4-f5fbc031f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_code(folder_path):\n",
    "    with open(folder_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    # Extract text before \"Example 1\" for the 'code_with_problem' column\n",
    "    df['question'] = df['code_with_problem'].apply(extract_text_before_example)\n",
    "\n",
    "    # Rename columns and create the dataset\n",
    "    df.rename(columns={'code_only': 'answer'}, inplace=True)\n",
    "    dataset = Dataset.from_pandas(df[['question', 'answer']])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7801c5e5-ca37-4247-b821-0e636bde1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = convert_folder_to_datasetdict_code('data/code/leetcode-solutions.json')\n",
    "train_dataset, test_dataset = train_test_split(code_dataset, test_size=0.2)\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03217e55-403e-49f4-809e-98c00df21937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1fe247-c841-4170-94c8-d97a89877b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = code_dict['train'].num_rows\n",
    "n_test = code_dict['test'].num_rows\n",
    "n_total = n_train + n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a43a410-aa83-46d1-a752-6e3b0387a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are given an integer array `nums` consisting of `n` elements, and an integer `k`.\\n\\nFind a contiguous subarray whose **length is greater than or equal to** `k` that has the maximum average value and return _this value_. Any answer with a calculation error less than `10-5` will be accepted.',\n",
       " 'answer': '```python\\ndef findMaxAverage(nums, k):\\n    total = sum(nums[:k])\\n    max_avg = total / k\\n    for i in range(k, len(nums)):\\n        total += nums[i] - nums[i - k]\\n        max_avg = max(max_avg, total / k)\\n    return max_avg\\n```\\n\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633e05f-5423-4e20-8c09-1346d19ef328",
   "metadata": {},
   "source": [
    "### Create general knowledge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b8c412-f8a8-4fa8-ab4a-5bea2ae894a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'data/general/general.csv'\n",
    "general = pd.read_csv(csv_file_path)\n",
    "general = general.sample(n=n_total, random_state=42)\n",
    "general['id'] = range(len(general))\n",
    "train_sample, test_sample = train_test_split(general, test_size=0.2, random_state=42)\n",
    "train_sample.reset_index(drop=True, inplace=True)\n",
    "test_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_sample[['question', 'answer']])\n",
    "test_dataset = Dataset.from_pandas(test_sample[['question', 'answer']])\n",
    "\n",
    "general_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "print(general_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57afb3e1-c17b-4ebe-846b-d82948014f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Add 1 letter to \"Iowa\" to get the name of this tribe who lived south of the Iowa',\n",
       " 'answer': 'Kiowa'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f352-aa2e-430f-b835-239807af143e",
   "metadata": {},
   "source": [
    "### Create math dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c35803-3c0b-44c0-900b-a5c70864de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_datasetdict_math(folder_path):\n",
    "    #data = {\"id\": [], \"question\": [], \"level\": [], \"type\": [], \"answer\": []}\n",
    "    data = {\"question\": [], \"answer\": []}\n",
    "\n",
    "    subject_dictionary = {\n",
    "        \"algebra\": 1,\n",
    "        \"counting_and_probability\": 2,\n",
    "        \"geometry\": 3,\n",
    "        \"intermediate_algebra\": 4,\n",
    "        \"number_theory\": 5,\n",
    "        \"prealgebra\": 6,\n",
    "        \"precalculus\": 7\n",
    "    }\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            folder_name = os.path.basename(os.path.normpath(subdir))\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                problem = json_data.get(\"problem\", \"\")\n",
    "                level = json_data.get(\"level\", \"\")\n",
    "                type_ = json_data.get(\"type\", \"\")\n",
    "                solution = json_data.get(\"solution\", \"\")\n",
    "\n",
    "                # Generate id from subject code and file name\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                subject_code = subject_dictionary.get(folder_name, 0)  # Default to 0 if not found\n",
    "                id_ = f\"{subject_code}_{file_name}\"\n",
    "\n",
    "                #data[\"id\"].append(id_)\n",
    "                data[\"question\"].append(problem)\n",
    "                #data[\"level\"].append(level)\n",
    "                #data[\"type\"].append(type_)\n",
    "                data[\"answer\"].append(solution)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee114fa9-509b-47d1-9039-ca0679324080",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train = convert_folder_to_datasetdict_math(\"data/math/train/\")\n",
    "math_test = convert_folder_to_datasetdict_math(\"data/math/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "820af25a-2806-4cbe-be23-107fc02f632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "math_train = math_train.shuffle(seed=42)\n",
    "math_train = math_train.select(range(n_train))\n",
    "\n",
    "math_test = math_test.shuffle(seed=42)\n",
    "math_test = math_test.select(range(n_test))\n",
    "\n",
    "math_dict = DatasetDict({\n",
    "    'train': math_train,\n",
    "    'test': math_test\n",
    "})\n",
    "\n",
    "print(math_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a46830-2716-495d-a669-87c046816932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the number of units in the distance between $(2,5)$ and $(-6,-1)$?',\n",
       " 'answer': 'We use the distance formula: $\\\\sqrt{(-6 - 2)^2 + (-1 - 5)^2},$ so then we find that $\\\\sqrt{64 + 36} = \\\\boxed{10}$.\\n\\n- OR -\\n\\nWe note that the points $(2, 5)$, $(-6, -1)$, and $(2, -1)$ form a right triangle with legs of length 6 and 8. This is a Pythagorean triple, so the length of the hypotenuse must be $\\\\boxed{10}$.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79018d-53d9-4042-98d7-3f715aea4ca9",
   "metadata": {},
   "source": [
    "### Create 50% samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e4c8b9-adfa-4f8a-a7ad-c0a7a3504f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train_sample = math_dict['train'].shuffle(seed=42).select([i for i in range(math_dict['train'].num_rows // 2)])\n",
    "math_test_sample = math_dict['test'].shuffle(seed=42).select([i for i in range(math_dict['test'].num_rows // 2)])\n",
    "\n",
    "general_train_sample = general_dict['train'].shuffle(seed=42).select([i for i in range(general_dict['train'].num_rows // 2+1)])\n",
    "general_test_sample = general_dict['test'].shuffle(seed=42).select([i for i in range(general_dict['test'].num_rows // 2)])\n",
    "\n",
    "code_train_sample = code_dict['train'].shuffle(seed=42).select([i for i in range(code_dict['train'].num_rows // 2)])\n",
    "code_test_sample = code_dict['test'].shuffle(seed=42).select([i for i in range(code_dict['test'].num_rows // 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74577ff-b47a-4c4a-bb30-779c72336122",
   "metadata": {},
   "source": [
    "### Create 50% general 50% math dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5cbcd89-e9ff-4f64-b38f-8b82f96a31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_math_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': math_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': math_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': math_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': math_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "\n",
    "general_math_dict = DatasetDict({\n",
    "    'train': general_math_dict['train'].shuffle(seed=42),\n",
    "    'test': general_math_dict['test'].shuffle(seed=42)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882a1fc2-3d94-4836-8a1d-c2eea04fd356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the monic quadratic polynomial, in $x,$ with real coefficients, which has $1 - i$ as a root.',\n",
       " 'answer': 'If a polynomial has real coefficients, then any complex conjugate of a root must also be a root.  Hence, the other root is $1 + i.$  Thus, the polynomial is\\n\\\\[(x - 1 - i)(x - 1 + i) = (x - 1)^2 - i^2 = \\\\boxed{x^2 - 2x + 2}.\\\\]'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_math_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b6262-6ee4-43e1-bd63-564fc5817642",
   "metadata": {},
   "source": [
    "### Create 50% general 50% code dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6fb1f1-c1f0-4860-9f28-e0229cde7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1887\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "general_code_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'question': code_train_sample['question'] + general_train_sample['question'],\n",
    "        'answer': code_train_sample['answer'] + general_train_sample['answer'],\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'question': code_test_sample['question'] + general_test_sample['question'],\n",
    "        'answer': code_test_sample['answer'] + general_test_sample['answer'],\n",
    "    })\n",
    "})\n",
    "\n",
    "general_code_dict = DatasetDict({\n",
    "    'train': general_code_dict['train'].shuffle(seed=42),\n",
    "    'test': general_code_dict['test'].shuffle(seed=42)\n",
    "})\n",
    "\n",
    "print(general_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc493dae-6232-483a-98b7-2413a0148097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '# You are given a 2D integer array `ranges` and two integers `left` and `right`. Each `ranges[i] = [starti, endi]` represents an **inclusive** interval between `starti` and `endi`.\\n\\nReturn `true` _if each integer in the inclusive range_ `[left, right]` _is covered by **at least one** interval in_ `ranges`. Return `false` _otherwise_.\\n\\nAn integer `x` is covered by an interval `ranges[i] = [starti, endi]` if `starti <= x <= endi`.',\n",
       " 'answer': '```python\\ndef isCovered(ranges: List[List[int]], left: int, right: int) -> bool:\\n    for i in range(left, right + 1):\\n        found = False\\n        for _range in ranges:\\n            if _range[0] <= i <= _range[1]:\\n                found = True\\n                break\\n        if not found:\\n            return False\\n    return True\\n```\\n\\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_code_dict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977fd4e-3fa5-4eff-b855-3af20d8d5288",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c905e62b-8fa0-40a1-8441-041309ba0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Please answer this question: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5355b55-7f8e-45a2-b6d1-91357968dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1887 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 1888.70 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 1377.98 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:01<00:00, 1378.70 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 1298.12 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 7385.19 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 7433.36 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 2077.31 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 2913.18 examples/s]\n",
      "Map: 100%|██████████| 1887/1887 [00:00<00:00, 1953.16 examples/s]\n",
      "Map: 100%|██████████| 472/472 [00:00<00:00, 2563.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset_math = math_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_code = code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general = general_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_code = general_code_dict.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_general_math = general_math_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efdd0266-9885-42ed-8a12-0de5a8cd1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd4453a-5b62-4c43-be98-911e0d460396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbb6b0-9938-4584-a913-d523357eb3c6",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9a639-1778-4713-abe2-945221bee480",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d461a46b-1029-4a02-9ecf-4bbde555e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_args(output_dir, L_RATE = 3e-4, BATCH_SIZE = 4, PER_DEVICE_EVAL_BATCH = 4, WEIGHT_DECAY = 0.01, SAVE_TOTAL_LIM = 3, NUM_EPOCHS = 3, OVERWRITE_OUTPUT_DIR = True, LOAD_BEST_MODEL_AT_END = True):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "       output_dir=output_dir,\n",
    "       overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "       save_strategy=\"epoch\",\n",
    "       evaluation_strategy=\"epoch\",\n",
    "       load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "       learning_rate=L_RATE,\n",
    "       per_device_train_batch_size=BATCH_SIZE,\n",
    "       per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "       weight_decay=WEIGHT_DECAY,\n",
    "       save_total_limit=SAVE_TOTAL_LIM,\n",
    "       num_train_epochs=NUM_EPOCHS,\n",
    "       predict_with_generate=True,\n",
    "       push_to_hub=False\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7a5c09-7624-49f3-992b-1b402c268694",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3e-3, 3e-4, 3e-5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21401d56-7d78-4038-96a3-f8e0f6b44525",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb02c39-54c8-4d83-baa1-0ffe30383c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231204_184137-s30tfz37</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/s30tfz37' target=\"_blank\">vivid-firebrand-17</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/s30tfz37' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/s30tfz37</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:26<02:52,  5.46it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:08<02:52,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8149617910385132, 'eval_rouge1': 0.10995362110086625, 'eval_rouge2': 0.03205449885714297, 'eval_rougeL': 0.09534439305275969, 'eval_rougeLsum': 0.10215049099534834, 'eval_runtime': 41.9023, 'eval_samples_per_second': 11.264, 'eval_steps_per_second': 2.816, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:26<02:50,  5.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3497, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:47<01:10,  6.69it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:31<01:10,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5980454683303833, 'eval_rouge1': 0.14648329059044826, 'eval_rouge2': 0.05500205586630799, 'eval_rougeL': 0.1257581518045653, 'eval_rougeLsum': 0.13622542966603407, 'eval_runtime': 43.3151, 'eval_samples_per_second': 10.897, 'eval_steps_per_second': 2.724, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:53<01:09,  6.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6592, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [06:09<00:00,  5.17it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [06:52<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5386518239974976, 'eval_rouge1': 0.13239823500131376, 'eval_rouge2': 0.048245988176787366, 'eval_rougeL': 0.11548947331316065, 'eval_rougeLsum': 0.12385955189580117, 'eval_runtime': 42.7494, 'eval_samples_per_second': 11.041, 'eval_steps_per_second': 2.76, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [07:00<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 422.5263, 'train_samples_per_second': 13.398, 'train_steps_per_second': 3.351, 'train_loss': 1.8174031144481595, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:26<02:54,  5.42it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:09<02:54,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7312365770339966, 'eval_rouge1': 0.15218676157844974, 'eval_rouge2': 0.06163434755852851, 'eval_rougeL': 0.13078325801480137, 'eval_rougeLsum': 0.14037009582489374, 'eval_runtime': 43.6071, 'eval_samples_per_second': 10.824, 'eval_steps_per_second': 2.706, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:23<02:54,  5.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2384, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:44<01:09,  6.76it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:27<01:09,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6180236339569092, 'eval_rouge1': 0.1419294322606442, 'eval_rouge2': 0.05933814874088656, 'eval_rougeL': 0.12402573039460824, 'eval_rougeLsum': 0.13327062586543964, 'eval_runtime': 42.3857, 'eval_samples_per_second': 11.136, 'eval_steps_per_second': 2.784, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:40<01:09,  5.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7438, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:55<00:00,  5.23it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [06:36<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.587369680404663, 'eval_rouge1': 0.15091141282307097, 'eval_rouge2': 0.06177349036870975, 'eval_rougeL': 0.1303818569072119, 'eval_rougeLsum': 0.140499861351289, 'eval_runtime': 41.1036, 'eval_samples_per_second': 11.483, 'eval_steps_per_second': 2.871, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:39<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 399.528, 'train_samples_per_second': 14.169, 'train_steps_per_second': 3.544, 'train_loss': 1.878079990882658, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:41<03:14,  4.85it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:22<03:14,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.173102855682373, 'eval_rouge1': 0.1509541489128586, 'eval_rouge2': 0.05838174564159471, 'eval_rougeL': 0.1268694329637305, 'eval_rougeLsum': 0.1389241976155938, 'eval_runtime': 41.4301, 'eval_samples_per_second': 11.393, 'eval_steps_per_second': 2.848, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:31<03:12,  4.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.861, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [04:02<01:16,  6.16it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:43<01:16,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0327463150024414, 'eval_rouge1': 0.14705630121860458, 'eval_rouge2': 0.056810334381106616, 'eval_rougeL': 0.12533656587980546, 'eval_rougeLsum': 0.1356657964352232, 'eval_runtime': 41.0923, 'eval_samples_per_second': 11.486, 'eval_steps_per_second': 2.872, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:57<01:16,  5.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.343, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [06:22<00:00,  4.64it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [07:03<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9992949962615967, 'eval_rouge1': 0.1474048317605285, 'eval_rouge2': 0.05774543918221979, 'eval_rougeL': 0.12583707187584028, 'eval_rougeLsum': 0.13708150892638965, 'eval_runtime': 40.9129, 'eval_samples_per_second': 11.537, 'eval_steps_per_second': 2.884, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [07:10<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 430.8929, 'train_samples_per_second': 13.138, 'train_steps_per_second': 3.286, 'train_loss': 2.502729642189155, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_math)\n",
    "    \n",
    "    output_dir_root = \"./results/math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_math = Seq2SeqTrainer(\n",
    "       model=model_math,\n",
    "       args=training_args_math,\n",
    "       train_dataset=tokenized_dataset_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_math.train()\n",
    "\n",
    "    del model_math, trainer_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841d885-c881-422c-ade2-90c26745c70d",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a56e89-3e6c-4155-8293-7f7a9bf44d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [04:18<07:42,  2.04it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [05:20<07:42,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5350375175476074, 'eval_rouge1': 0.11721102782605994, 'eval_rouge2': 0.034336015307031405, 'eval_rougeL': 0.11502648637877302, 'eval_rougeLsum': 0.11501143176245161, 'eval_runtime': 62.031, 'eval_samples_per_second': 7.609, 'eval_steps_per_second': 1.902, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [05:54<08:08,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.061, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [10:10<03:55,  2.00it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [11:05<03:55,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.264697790145874, 'eval_rouge1': 0.10659240077007925, 'eval_rouge2': 0.03670836595815304, 'eval_rougeL': 0.10468458144177192, 'eval_rougeLsum': 0.10468716007322851, 'eval_runtime': 55.0834, 'eval_samples_per_second': 8.569, 'eval_steps_per_second': 2.142, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [11:37<01:08,  6.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3332, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [15:03<00:00,  3.33it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [15:57<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1811020374298096, 'eval_rouge1': 0.10671584122236769, 'eval_rouge2': 0.038957483768928686, 'eval_rougeL': 0.10457401587359638, 'eval_rougeLsum': 0.10459596143137674, 'eval_runtime': 53.9787, 'eval_samples_per_second': 8.744, 'eval_steps_per_second': 2.186, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [16:03<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 963.0746, 'train_samples_per_second': 5.878, 'train_steps_per_second': 1.47, 'train_loss': 1.5088243861656405, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [05:31<08:43,  1.80it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [06:28<08:43,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4910694360733032, 'eval_rouge1': 0.11911878258078282, 'eval_rouge2': 0.0444059650080581, 'eval_rougeL': 0.11698277072628263, 'eval_rougeLsum': 0.11654976093618492, 'eval_runtime': 57.2262, 'eval_samples_per_second': 8.248, 'eval_steps_per_second': 2.062, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 502/1416 [07:04<06:18,  2.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0653, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [12:02<06:30,  1.21it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [13:00<06:30,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.328734040260315, 'eval_rouge1': 0.11465649503651226, 'eval_rouge2': 0.046958915863290096, 'eval_rougeL': 0.11268805229691237, 'eval_rougeLsum': 0.11251093973210091, 'eval_runtime': 57.9208, 'eval_samples_per_second': 8.149, 'eval_steps_per_second': 2.037, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [13:51<01:11,  5.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4615, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [18:42<00:00,  2.25it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [19:40<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2787977457046509, 'eval_rouge1': 0.11488158867090989, 'eval_rouge2': 0.04650912829189177, 'eval_rougeL': 0.11301840310581476, 'eval_rougeLsum': 0.11313591044535914, 'eval_runtime': 58.0379, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [19:50<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1190.0221, 'train_samples_per_second': 4.757, 'train_steps_per_second': 1.19, 'train_loss': 1.6281611986752957, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [12:07<19:45,  1.26s/it]  c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [14:13<19:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1649770736694336, 'eval_rouge1': 0.12094778151993357, 'eval_rouge2': 0.041445618743021245, 'eval_rougeL': 0.11924863436296773, 'eval_rougeLsum': 0.1191366371153875, 'eval_runtime': 126.3156, 'eval_samples_per_second': 3.737, 'eval_steps_per_second': 0.934, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 500/1416 [15:17<23:27,  1.54s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.939, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [26:30<12:25,  1.58s/it]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [28:35<12:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9423874616622925, 'eval_rouge1': 0.1140291826879622, 'eval_rouge2': 0.04396361155780263, 'eval_rougeL': 0.11122934139816165, 'eval_rougeLsum': 0.11116114169005785, 'eval_runtime': 125.1329, 'eval_samples_per_second': 3.772, 'eval_steps_per_second': 0.943, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1000/1416 [30:12<06:28,  1.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2665, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [40:58<00:00,  1.15s/it]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [43:04<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8862913846969604, 'eval_rouge1': 0.11416862090139981, 'eval_rouge2': 0.043537742104730416, 'eval_rougeL': 0.11160464533282688, 'eval_rougeLsum': 0.1115809059668732, 'eval_runtime': 125.6944, 'eval_samples_per_second': 3.755, 'eval_steps_per_second': 0.939, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [43:09<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2589.9732, 'train_samples_per_second': 2.186, 'train_steps_per_second': 0.547, 'train_loss': 2.470928213690634, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_code)\n",
    "    \n",
    "    output_dir_root = \"./results/code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_code = Seq2SeqTrainer(\n",
    "       model=model_code,\n",
    "       args=training_args_code,\n",
    "       train_dataset=tokenized_dataset_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_code.train()\n",
    "\n",
    "    del model_code, trainer_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35f10-0a68-4a87-a967-c296629f6b32",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bb296-b51f-4715-9b3d-d5800246c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general)\n",
    "\n",
    "    output_dir_root = \"./results/general\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general = Seq2SeqTrainer(\n",
    "       model=model_general,\n",
    "       args=training_args_general,\n",
    "       train_dataset=tokenized_dataset_general[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general.train()\n",
    "\n",
    "    del model_general, trainer_general\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b818c67-6b6b-4ce3-b43a-9a5eb3d57695",
   "metadata": {},
   "source": [
    "### General + Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39dc5804-50f2-46e0-9192-971b0a153f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231204_220058-92x30oln</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln' target=\"_blank\">lilac-planet-18</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/92x30oln</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:05<02:13,  7.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [01:43<02:13,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.132077693939209, 'eval_rouge1': 0.055051808944643704, 'eval_rouge2': 0.016222703488948487, 'eval_rougeL': 0.05433968281931898, 'eval_rougeLsum': 0.05441902197562565, 'eval_runtime': 38.6494, 'eval_samples_per_second': 12.212, 'eval_steps_per_second': 3.053, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:10<02:00,  7.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6281, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:10<01:12,  6.51it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:46<01:12,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8248320817947388, 'eval_rouge1': 0.07404124374521334, 'eval_rouge2': 0.019232725796420602, 'eval_rougeL': 0.07357504130459036, 'eval_rougeLsum': 0.07368692572109758, 'eval_runtime': 36.6937, 'eval_samples_per_second': 12.863, 'eval_steps_per_second': 3.216, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:05<00:53,  7.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7786, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:00<00:00,  7.48it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [05:37<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.736644983291626, 'eval_rouge1': 0.07237803222073752, 'eval_rouge2': 0.020723353671982178, 'eval_rougeL': 0.07175227425435451, 'eval_rougeLsum': 0.07182516121018465, 'eval_runtime': 36.3015, 'eval_samples_per_second': 13.002, 'eval_steps_per_second': 3.251, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:48<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 351.6636, 'train_samples_per_second': 16.098, 'train_steps_per_second': 4.027, 'train_loss': 1.9456297017760196, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:03<01:56,  8.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [01:38<01:56,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.760346531867981, 'eval_rouge1': 0.12132619838044889, 'eval_rouge2': 0.0371879520789538, 'eval_rougeL': 0.1208510402218295, 'eval_rougeLsum': 0.1210844252051826, 'eval_runtime': 34.9811, 'eval_samples_per_second': 13.493, 'eval_steps_per_second': 3.373, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [01:49<01:55,  7.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3452, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [02:47<01:11,  6.58it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:23<01:11,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5788134336471558, 'eval_rouge1': 0.11538274777158411, 'eval_rouge2': 0.03491347388730446, 'eval_rougeL': 0.11516147987108587, 'eval_rougeLsum': 0.11438624150784589, 'eval_runtime': 36.8886, 'eval_samples_per_second': 12.795, 'eval_steps_per_second': 3.199, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [03:42<00:53,  7.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.659, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [04:37<00:00,  7.26it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [05:14<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5343466997146606, 'eval_rouge1': 0.12369657569645576, 'eval_rouge2': 0.038815463721845155, 'eval_rougeL': 0.12219236045938289, 'eval_rougeLsum': 0.12186390364948171, 'eval_runtime': 36.8111, 'eval_samples_per_second': 12.822, 'eval_steps_per_second': 3.206, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 322.2697, 'train_samples_per_second': 17.566, 'train_steps_per_second': 4.394, 'train_loss': 1.8327802668857036, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:06<02:24,  6.52it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:41<02:24,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3541741371154785, 'eval_rouge1': 0.10543474610190923, 'eval_rouge2': 0.027912390306222754, 'eval_rougeL': 0.104541700149297, 'eval_rougeLsum': 0.10379574014373033, 'eval_runtime': 35.2405, 'eval_samples_per_second': 13.394, 'eval_steps_per_second': 3.348, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [01:59<02:44,  5.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1622, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:04<01:12,  6.55it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [03:40<01:12,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1470742225646973, 'eval_rouge1': 0.11440688184860422, 'eval_rouge2': 0.03585640348614795, 'eval_rougeL': 0.11380251997686144, 'eval_rougeLsum': 0.11309426185751145, 'eval_runtime': 36.0755, 'eval_samples_per_second': 13.084, 'eval_steps_per_second': 3.271, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:03<01:09,  5.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4896, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:18<00:00,  6.22it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:54<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0940744876861572, 'eval_rouge1': 0.11471425599899554, 'eval_rouge2': 0.03639041958518499, 'eval_rougeL': 0.11419256284976584, 'eval_rougeLsum': 0.11350022159156378, 'eval_runtime': 36.0568, 'eval_samples_per_second': 13.09, 'eval_steps_per_second': 3.273, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:11<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 370.9641, 'train_samples_per_second': 15.26, 'train_steps_per_second': 3.817, 'train_loss': 2.68684063927602, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [06:11<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general_code = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_code = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_code)\n",
    "\n",
    "    output_dir_root = \"./results/general_code\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_code = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_code = Seq2SeqTrainer(\n",
    "       model=model_general_code,\n",
    "       args=training_args_general_code,\n",
    "       train_dataset=tokenized_dataset_general_code[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_code[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_code,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_code.train()\n",
    "\n",
    "    del model_general_code, trainer_general_code\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843366a-5d48-4364-bc58-4cccc0f2c5c8",
   "metadata": {},
   "source": [
    "### General + Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c5b87f6-d73f-4aa4-bc80-28ee764a0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myezixuanclara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yezix\\Desktop\\2023_Fall\\MIT6.8610\\project\\git\\wandb\\run-20231204_222254-yv29x7g0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0' target=\"_blank\">good-jazz-19</a></strong> to <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yezixuanclara/huggingface' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0' target=\"_blank\">https://wandb.ai/yezixuanclara/huggingface/runs/yv29x7g0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:30<02:35,  6.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 33%|███▎      | 472/1416 [02:07<02:35,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5402798652648926, 'eval_rouge1': 0.032277776555268506, 'eval_rouge2': 0.0029141143878364728, 'eval_rougeL': 0.03204248391573171, 'eval_rougeLsum': 0.031950849918273065, 'eval_runtime': 37.0247, 'eval_samples_per_second': 12.748, 'eval_steps_per_second': 3.187, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:32<02:17,  6.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0606, 'learning_rate': 0.001940677966101695, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:39<01:17,  6.08it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [04:16<01:17,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3229095935821533, 'eval_rouge1': 0.032444724403872735, 'eval_rouge2': 0.0035144391149949, 'eval_rougeL': 0.03171422707637668, 'eval_rougeLsum': 0.03172260187056673, 'eval_runtime': 36.9298, 'eval_samples_per_second': 12.781, 'eval_steps_per_second': 3.195, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:32<01:09,  6.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2633, 'learning_rate': 0.0008813559322033899, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:35<00:00,  6.53it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|██████████| 1416/1416 [06:11<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.247628927230835, 'eval_rouge1': 0.039121258214399816, 'eval_rouge2': 0.0032839848504259083, 'eval_rougeL': 0.03872789528552882, 'eval_rougeLsum': 0.038698280799125157, 'eval_runtime': 35.8744, 'eval_samples_per_second': 13.157, 'eval_steps_per_second': 3.289, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [06:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 385.1121, 'train_samples_per_second': 14.7, 'train_steps_per_second': 3.677, 'train_loss': 2.4215501257255254, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:12<02:38,  5.97it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:48<02:38,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9895418882369995, 'eval_rouge1': 0.1320215180700664, 'eval_rouge2': 0.03874952300747349, 'eval_rougeL': 0.12353501415658114, 'eval_rougeLsum': 0.1275214497003628, 'eval_runtime': 36.0791, 'eval_samples_per_second': 13.082, 'eval_steps_per_second': 3.271, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:00<02:13,  6.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4809, 'learning_rate': 0.00019406779661016945, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:07<01:12,  6.51it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|██████▋   | 944/1416 [03:44<01:12,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8589943647384644, 'eval_rouge1': 0.135461050614511, 'eval_rouge2': 0.038585915833919224, 'eval_rougeL': 0.12459804955343722, 'eval_rougeLsum': 0.12869890674568712, 'eval_runtime': 36.548, 'eval_samples_per_second': 12.915, 'eval_steps_per_second': 3.229, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [03:55<01:08,  6.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9168, 'learning_rate': 8.813559322033898e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [04:58<00:00,  6.85it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:35<00:00,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8303388357162476, 'eval_rouge1': 0.14259260962717454, 'eval_rouge2': 0.04300571794768096, 'eval_rougeL': 0.13308619842145114, 'eval_rougeLsum': 0.1364182526233655, 'eval_runtime': 36.3798, 'eval_samples_per_second': 12.974, 'eval_steps_per_second': 3.244, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:42<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 342.522, 'train_samples_per_second': 16.527, 'train_steps_per_second': 4.134, 'train_loss': 2.0566785995569608, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 472/1416 [01:12<02:35,  6.06it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|███▎      | 472/1416 [01:48<02:35,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3861546516418457, 'eval_rouge1': 0.1119125107451445, 'eval_rouge2': 0.0407551038633739, 'eval_rougeL': 0.10108785562620667, 'eval_rougeLsum': 0.10601038009687778, 'eval_runtime': 36.2431, 'eval_samples_per_second': 13.023, 'eval_steps_per_second': 3.256, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 501/1416 [02:01<02:24,  6.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0762, 'learning_rate': 1.9406779661016948e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 944/1416 [03:09<01:13,  6.42it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 67%|██████▋   | 944/1416 [03:46<01:13,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2488908767700195, 'eval_rouge1': 0.12060288745848656, 'eval_rouge2': 0.039178581996189996, 'eval_rougeL': 0.11036107868460496, 'eval_rougeLsum': 0.11576035112598533, 'eval_runtime': 37.3415, 'eval_samples_per_second': 12.64, 'eval_steps_per_second': 3.16, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1001/1416 [04:03<01:13,  5.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5417, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416/1416 [05:09<00:00,  6.60it/s]c:\\Users\\yezix\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 1416/1416 [05:46<00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2149879932403564, 'eval_rouge1': 0.11827782446039363, 'eval_rouge2': 0.0367991779285983, 'eval_rougeL': 0.1077602373969512, 'eval_rougeLsum': 0.11287944637073197, 'eval_runtime': 37.475, 'eval_samples_per_second': 12.595, 'eval_steps_per_second': 3.149, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1416/1416 [05:49<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 349.4659, 'train_samples_per_second': 16.199, 'train_steps_per_second': 4.052, 'train_loss': 2.6996212544414284, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model_general_math = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    data_collator_general_math = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_general_math)\n",
    "\n",
    "    output_dir_root = \"./results/general_math\"\n",
    "    output_dir = f\"{output_dir_root}/{lr:.0e}\".replace(\"0\", \"\")\n",
    "    training_args_general_math = training_args(L_RATE = lr, BATCH_SIZE = 4, NUM_EPOCHS = 3, output_dir=output_dir)\n",
    "\n",
    "    trainer_general_math = Seq2SeqTrainer(\n",
    "       model=model_general_math,\n",
    "       args=training_args_general_math,\n",
    "       train_dataset=tokenized_dataset_general_math[\"train\"], \n",
    "       eval_dataset=tokenized_dataset_general_math[\"test\"],   \n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator_general_math,\n",
    "       compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer_general_math.train()\n",
    "\n",
    "    del model_general_math, trainer_general_math\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
